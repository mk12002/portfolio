{
  "projects": [
    {
      "slug": "hybex-law",
      "title": "HybEx-Law: Hybrid Legal AI Engine",
      "category": "Legal AI",
      "tagline": "LegalBERT + GNN + Prolog hybrid reasoning system",
      "metric": "F1 Score: 0.985",
      "precision": "98.48%",
      "tags": ["Legal NLP", "Symbolic AI", "GNN", "Hybrid Models", "Prolog", "LegalBERT"],
      "description": "A hybrid AI system integrating LegalBERT embeddings, GNN-based graph reasoning, and Prolog rule-based symbolic execution for legal aid eligibility determination.",
      "overview": "HybEx-Law is a hybrid neural-symbolic AI system designed to automatically determine legal-aid eligibility under the Indian Legal Services Authorities Act. The project addresses a complex classification problem that requires both linguistic understanding and strict rule compliance. The system processes free-form queries written by citizens, extracts structured signals, interprets statutory thresholds, and produces eligibility decisions supplemented by confidence calibration and explicit reasoning tracebacks. HybEx-Law achieves state-of-the-art performance with an F1 of 0.985, confirming that hybrid architectures are superior to purely neural or purely symbolic models for high-stakes, rule-governed tasks.",
      "problem": "Eligibility for legal aid in India is determined by a combination of income-based thresholds, membership in vulnerable categories (e.g., women, seniors, disabled persons), nature of legal domain (criminal, civil, family, constitutional), and contextual constraints within statutory rules. However, real-world user queries often contain fragmented, unstructured, or ambiguous descriptions of their situation. Purely rule-based systems cannot interpret natural language, while deep models tend to miss legally critical details or misclassify borderline cases. Therefore, the key problem is to design a system that can understand user language, extract structured legal concepts, apply statutory rules deterministically, resolve uncertainty using calibrated model ensembles, and produce interpretable, audit-ready decisions. HybEx-Law solves this by combining neural and symbolic reasoning into a single inference pipeline.",
      "motivation": [
        "Access to justice in India relies heavily on timely legal-aid screening. Manual screening is prone to inconsistency, inter-reviewer variability, and excessive processing delays. While ML-based systems have been proposed in legal tech, they often lack interpretability, which is unacceptable for judicial applications.",
        "The motivation behind HybEx-Law comes from three observations: First, no standardized dataset exists for legal-aid eligibility. A synthetic yet legally aligned dataset had to be created, following statutory sources and expert interpretation. This dataset includes user queries, domain cues, demographic values, and rule-grounded eligibility labels.",
        "Second, purely neural systems lack explainability. Legal processes require judicial reasoning, not black-box predictions. Any automated decision must be traceable to a clear legal rationale.",
        "Third, symbolic systems alone cannot handle messy, real-world language. Users express eligibility factors in many forms, requiring language understanding beyond hard-coded patterns.",
        "HybEx-Law leverages the strengths of both worlds—neural contextualization and symbolic rule enforcement—to build a system suitable for high-precision judicial workflows."
      ],
      "keyMetrics": [
        {"name": "Accuracy", "value": "98.48%"},
        {"name": "Precision", "value": "98.31%"},
        {"name": "Recall", "value": "98.70%"},
        {"name": "F1-Score", "value": "0.985"}
      ],
      "metricsDescription": "These metrics indicate highly reliable decision-making even on borderline cases involving partial eligibility, ambiguous phrasing, or multiple demographic attributes.",
      "approach": "HybEx-Law uses a multi-layered inference methodology combining parallel model execution with symbolic reasoning and confidence calibration.",
      "approachSteps": [
        "Preprocessing: Converts raw user queries + structured inputs into model-ready formats.",
        "Parallel Model Execution: EnhancedLegalBERT performs multi-task prediction. Two auxiliary neural models refine domain and eligibility predictions. A GNN encodes rule-based relationships. Prolog evaluates statutory conditions deterministically.",
        "Confidence Calibration: All model outputs undergo Isotonic Regression to ensure that prediction probabilities reflect true likelihood.",
        "Weighted Ensemble Fusion: A meta-learner combines the five calibrated outputs into a single eligibility score.",
        "Symbolic Override: If Prolog identifies statutory triggers (e.g., senior citizen, disabled person), it overrides neural predictions.",
        "Final Decision + Justification: The system produces a JSON output with eligibility, reasoning trace, calibrated confidence, and relevant rule citations."
      ],
      "flowDiagram": {
        "src": "/projects/hybex-law/overall.png",
        "caption": "End-to-End Hybrid Inference Pipeline of HybEx-Law",
        "alt": "System architecture diagram showing data preprocessing, parallel model execution, confidence calibration, and final decision output"
      },
      "architecture": "Text Path (neural models) + Structured Facts Path (GNN + Prolog) → Hybrid Orchestrator → Confidence Calibration → Weighted Ensemble → Final Decision",
      "architectureDescription": "The architecture consists of five independent models unified by a hybrid orchestrator that manages conflict resolution, weighting, calibration, and final decision synthesis.",
      "architectureComponents": [
        {
          "name": "EnhancedLegalBERT (MTL)",
          "description": "Produces contextual embeddings and predictions for eligibility, domain, and auxiliary signals using multi-task learning."
        },
        {
          "name": "EligibilityPredictor",
          "description": "Binary classifier specialized for threshold and demographic cues."
        },
        {
          "name": "DomainClassifier",
          "description": "Handles multi-label classification across several legal domains."
        },
        {
          "name": "LegalGAT",
          "description": "Encodes rule-entity relationships using graph attention layers."
        },
        {
          "name": "Prolog Engine",
          "description": "Houses statutory rules and returns deterministic judgments."
        }
      ],
      "architectureDiagram": {
        "src": "/projects/hybex-law/ensemble-architecture.png",
        "caption": "Overview of the hybrid ensemble pipeline combining text-based neural models, structured-fact reasoning through GNN and Prolog, and unified ensemble inputs for final eligibility prediction.",
        "alt": "Ensemble architecture diagram showing integration of neural models, GNN, and Prolog reasoning"
      },
      "modelArchitecture": [
        {
          "title": "EnhancedLegalBERT",
          "content": "Extends LegalBERT with multi-task heads, attention-based pooling, and three-output architecture (eligibility/domain/auxiliary). Better handling of statutory language and edge cases."
        },
        {
          "title": "Auxiliary Predictors",
          "content": "Two neural models independently learn domain cues, income thresholds, and vulnerability category markers."
        },
        {
          "title": "LegalGAT",
          "content": "Transforms the Prolog rule base into a graph. Nodes include income thresholds, roles (woman, child, disabled, etc.), legal domains, and case-specific entities. Edges encode rule interactions, enabling relational reasoning."
        },
        {
          "title": "Prolog Engine",
          "content": "Encodes mandatory eligibility rules, short-circuit conditions, exceptions and overrides. This model guarantees explainability and ensures that legal requirements are never violated."
        }
      ],
      "algorithmDetails": [
        {
          "name": "Prediction Algorithm",
          "description": "End-to-end inference pipeline combining neural and symbolic reasoning.",
          "steps": [
            "Tokenize query → generate contextual embeddings.",
            "Extract demographic and domain cues.",
            "Construct a case-specific rule graph.",
            "Evaluate Prolog rules for deterministic constraints.",
            "Run Transformer + neural classifiers + GNN.",
            "Calibrate all probability distributions.",
            "Fuse outputs using optimized ensemble weights.",
            "Generate final eligibility + rule rationale."
          ]
        },
        {
          "name": "Calibration Algorithm",
          "description": "Isotonic Regression ensures that predicted probabilities reflect true model reliability. This significantly improves decision trustworthiness, especially for borderline cases."
        },
        {
          "name": "Interpretation Algorithm",
          "description": "Combines Prolog trace output, dominant model predictions, and rule activations to produce a machine-readable reasoning summary."
        }
      ],
      "datasetInfo": {
        "description": "Because no public legal-aid dataset exists, a synthetic, legally aligned dataset was built. It includes natural language queries generated through templates + paraphrasing, demographic variables, income levels, legal domain categories, rule-based eligibility labels, and extracted tokens/entities.",
        "details": [
          "Covers 5 legal domains with realistic distributions",
          "Special attention to rare and borderline cases",
          "Incomes only slightly above legal thresholds",
          "Multiple overlapping vulnerability markers",
          "Large enough to train Transformers and GNNs",
          "Diversity preserved through controlled sampling"
        ]
      },
      "trainingProcess": [
        {
          "name": "Neural Models",
          "details": [
            "Optimizer: AdamW",
            "Learning Rate Warmup + linear decay",
            "Mixed precision (FP16) for efficiency",
            "Multi-task loss balancing for EnhancedLegalBERT"
          ]
        },
        {
          "name": "GNN Model",
          "details": [
            "Graph edges derived from Prolog rule relationships",
            "Trained using supervised graph-level classification",
            "Regularized with dropout and attention normalization"
          ]
        },
        {
          "name": "Ensemble Calibration",
          "details": [
            "Post-training calibration using validation partitions",
            "Isotonic Regression improves confidence reliability",
            "Ensemble weights tuned for F1-maximization"
          ]
        }
      ],
      "results": [
        "HybEx-Law achieved near-perfect reliability and produced transparent results suitable for legal workflows. The hybrid approach significantly outperforms neural-only models by capturing statutory rule dependencies.",
        "Prolog overrides reduce false positives on sensitive categories. GNN contributes relational understanding not captured by text models. Calibrated outputs reduce overconfidence and allow safer deployment.",
        "The system is suitable for legal aid centers, public-facing portals, and NGO triage systems requiring fairness and explainability."
      ],
      "resultsHighlights": [
        {"label": "Best Configuration", "value": "Full hybrid ensemble (5 models)"},
        {"label": "Performance Gain", "value": "+0.003 F1 over neural-only"},
        {"label": "Rule Coverage", "value": "100% statutory compliance"},
        {"label": "Inference Speed", "value": "Sub-second per query"}
      ],
      "performanceMetrics": {
        "description": "Performance is evaluated across all combinations of the 5 models, demonstrating how performance steadily increases as more reasoning types (neural + symbolic) are combined.",
        "figures": [
          {
            "src": "/projects/hybex-law/overall_graph.png",
            "caption": "Accuracy, precision, recall, and F1-score comparison across all model combinations, ranked by overall performance, highlighting the superiority of the full hybrid ensemble compared to smaller or single-model approaches.",
            "alt": "Standalone model performance comparison graph"
          },
          {
            "src": "/projects/hybex-law/precision recall trade off.png",
            "caption": "Precision–recall distribution of all 17 model combinations, illustrating that the full hybrid ensemble achieves the best balance of high precision and high recall, outperforming both neural-only and symbolic-only systems.",
            "alt": "PR-curves for all 17 ensembles"
          },
          {
            "src": "/projects/hybex-law/performance vs. ensamble complexity.png",
            "caption": "Relationship between ensemble size and F1-score, demonstrating that performance improves as more complementary models are added, with the hybrid 5-model ensemble achieving the highest reliability.",
            "alt": "Performance vs. Ensemble Size"
          },
          {
            "src": "/projects/hybex-law/performance by ensemble type.png",
            "caption": "Comparison of F1-scores across single models, neural-only ensembles, and hybrid ensembles, showing consistent performance gains when combining neural and symbolic reasoning components.",
            "alt": "Comparison across Ensemble Families"
          }
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "Neural Models Alone",
          "points": [
            "High accuracy, but miss strict rule triggers (e.g., senior citizen override).",
            "Sensitive to paraphrasing and ambiguous phrasing."
          ]
        },
        {
          "approach": "Symbolic System Alone",
          "points": [
            "Perfect precision on rule-trigger cases.",
            "Cannot infer context or paraphrased expressions."
          ]
        },
        {
          "approach": "GNN Alone",
          "points": [
            "Correctly identifies relational compositions (e.g., income + vulnerability).",
            "Needs textual cues for categorical boundaries."
          ]
        }
      ],
      "comparativeConclusion": "Hybrid System: Combining all five models resolves all weaknesses. Neural handles language, Symbolic ensures rule consistency, GNN reasons over structured relationships. This architecture most closely resembles real human legal reasoning.",
      "ablationStudies": {
        "description": "The ablation study evaluates 17 different model combinations, showing how each component contributes to the final system performance.",
        "findings": [
          "Full hybrid ensemble → best F1 (0.985)",
          "Neural-only ensembles → good but inferior on rule-heavy cases",
          "Adding GNN → improves performance on threshold edge cases",
          "Symbolic override → significantly reduces eligibility misclassification"
        ],
        "tableImage": {
          "src": "/projects/hybex-law/ablation_table.png",
          "caption": "Table: Ablation study results comparing 17 model combinations, showing that the full hybrid ensemble achieves the highest performance (F1 = 0.9850) over all neural, symbolic, and mixed configurations.",
          "alt": "Ablation study table with 17 model combinations ranked by F1-score"
        }
      },
      "useCases": [
        {
          "title": "Legal Aid Centers",
          "description": "Automate initial triage and reduce workload for legal aid officers."
        },
        {
          "title": "Government Portals",
          "description": "Provide real-time eligibility checks to citizens through online platforms."
        },
        {
          "title": "Judicial Support Tools",
          "description": "Ensure consistent screening and documentation for court proceedings."
        },
        {
          "title": "NGOs & Nonprofits",
          "description": "Provide reliable pre-screening in remote or underserved areas."
        }
      ],
      "techStack": ["Python", "PyTorch", "LegalBERT", "Graph Neural Networks", "SWI-Prolog", "Transformers", "Sklearn (Calibration)"],
      "githubUrl": "https://github.com/mk12002/HybEx-Law",
      "liveUrl": "",
      "status": "active",
      "type": "research"
    },
    {
      "slug": "nexus",
      "title": "Nexus: A Three-Pronged Agentic AI System for Intelligent Research & Analysis",
      "category": "Reasoning",
      "tagline": "3-agent system for automated research discovery and synthesis",
      "metric": "90% Time Reduction",
      "tags": ["Multi-Agent", "NLP Automation", "LangGraph", "Flask", "AI Research"],
      "publicationUrl": "https://app.readytensor.ai/publications/nexus-a-three-pronged-agentic-ai-system-for-intelligent-research-and-analysis-Y06tMJMVmNjI",
      "overview": "Nexus is an advanced multi-agent AI research and analysis system designed to automate the end-to-end workflow of information gathering, fact-verification, structured article generation, and multi-format storage. Traditional research workflows require rigorous manual searching, filtering, validation, note-making, and synthesis — processes that are time-consuming, cognitively heavy, and prone to omissions or inconsistencies. Nexus eliminates these inefficiencies through an orchestrated pipeline of three highly specialized AI agents, each responsible for one phase of the research lifecycle. The system integrates web scraping (Google Search API, Wikipedia API, ArXiv API), natural language processing, fact-checking, document generation, and agentic reasoning into a seamless fully automated process. Built with Flask, LangGraph, React, and Tailwind, Nexus is optimized for both backend efficiency and frontend usability. It supports tone customization, multi-format exports (.md, .json, .pdf), and includes an interactive UI for monitoring research progress.",
      "homepageUI": {
        "src": "/projects/nexus/front.jpg",
        "caption": "Nexus Homepage UI: \"Research Powered by AI\" interface showcasing user query input and capabilities overview.",
        "alt": "Nexus homepage interface with query input"
      },
      "problem": "Modern research faces several persistent challenges: Information Overload — With millions of documents available online, identifying accurate, relevant, and high-quality material becomes increasingly difficult. Manual Verification Complexity — Searching multiple sources (Google, Wikipedia, ArXiv), comparing data points, and performing fact-checking require significant time and expertise. Unstructured Results — Raw search results rarely provide structured narratives. Researchers must invest additional effort in synthesizing content into readable, coherent formats. Inconsistent Quality — Human-generated summaries depend heavily on individual skill, experience, and cognitive load. Nexus addresses these challenges by automating the entire cycle, ensuring consistency, reliability, and structured outputs.",
      "motivation": [
        "The explosion of online information and advancements in generative AI have created an opportunity for intelligent systems that can understand, organize, and present information autonomously. However, classical search engines provide raw results, not structured insights; generative models alone cannot verify information, leading to hallucinations; and manual research is slow and resource-intensive.",
        "The motivation behind Nexus is to bridge these gaps by combining retrieval-based factual accuracy, agentic workflow orchestration, AI-driven content synthesis, and automated storage pipelines. This system aims to reduce research time from hours to seconds while maintaining high accuracy and readability."
      ],
      "keyMetrics": [
        {"name": "Average Processing Time", "value": "<8 seconds", "description": "End-to-end time from topic input to PDF generation"},
        {"name": "Sources per Query", "value": "3–10", "description": "Configurable retrieval from Google, Wikipedia, ArXiv, Fact-Check APIs"},
        {"name": "Article Structure", "value": "6 sections", "description": "Title → Introduction → Key Insights → Industry Impact → Future Prospects → Conclusion"},
        {"name": "Fact-Check Coverage", "value": "80–100%", "description": "Integrated verification depending on topic availability"}
      ],
      "approach": "Nexus uses a three-phase agentic methodology: Phase 1 — Data Retrieval (Research Agent) aggregates raw information from multiple trusted sources including Google Search results, Wikipedia summaries, ArXiv academic papers, and fact-checking results. Phase 2 — Content Synthesis (Reporting Agent) transforms the aggregated data into a structured article with logical flow, section separation, professional formatting, and tone customization. Phase 3 — Storage & Export (Storage Agent) outputs the research in Markdown (.md), JSON (.json), and PDF formats using WeasyPrint.",
      "approachSteps": [
        {
          "step": 1,
          "name": "Research Agent: Data Retrieval",
          "description": "Aggregates raw information from Google Search, Wikipedia, ArXiv, and Fact-Check APIs"
        },
        {
          "step": 2,
          "name": "Reporting Agent: Content Synthesis",
          "description": "Transforms aggregated data into structured articles with tone customization"
        },
        {
          "step": 3,
          "name": "Storage Agent: Export",
          "description": "Outputs research in Markdown, JSON, and PDF formats"
        }
      ],
      "researchProcessingUI": {
        "src": "/projects/nexus/processing3.jpg",
        "caption": "Research Processing Interface: Progress bar showing initialization → searching → synthesizing → complete.",
        "alt": "Research processing progress interface"
      },
      "structuredResultsUI": {
        "src": "/projects/nexus/result.jpg",
        "caption": "Structured Results Display: AI-generated article format with clean segmentation.",
        "alt": "Structured article results display"
      },
      "architecture": [
        {
          "tier": "Tier 1 - Presentation Layer",
          "description": "Built using React + Tailwind CSS. Includes live progress tracking and article preview. Allows tone customization and exports."
        },
        {
          "tier": "Tier 2 - Application Layer",
          "description": "Flask + LangGraph core orchestration backbone: Manages HTTP requests, executes the Research → Reporting → Storage pipeline, implements LangGraph state transitions ensuring ordering and determinism."
        },
        {
          "tier": "Tier 3 - Data Layer",
          "description": "Stores Markdown reports, JSON data structures, and PDFs. Provides persistent reproducibility."
        }
      ],
      "settingsPageUI": {
        "src": "/projects/nexus/settings.jpg",
        "caption": "Settings Page: Controls for depth, maximum sources, and recency preferences.",
        "alt": "Settings page with configuration controls"
      },
      "architectureDescription": "Nexus follows a modular three-tier architecture, each layer designed for reliability and scalability. The frontend provides an intuitive interface for query input and result visualization, the application layer orchestrates the multi-agent workflow using LangGraph, and the data layer ensures persistent storage across multiple formats.",
      "modelArchitecture": [
        {
          "name": "Retrieval Subsystem",
          "description": "Responsible for calling Google Search API, Wikipedia API, ArXiv API, and Google Fact Check API to gather comprehensive source material."
        },
        {
          "name": "Generative Subsystem",
          "description": "Uses Gemini / LLM backend to synthesize structured articles from aggregated research data."
        },
        {
          "name": "Workflow Subsystem",
          "description": "Uses LangGraph to enforce deterministic agent sequencing: research_agent → reporting_agent → storage_agent."
        }
      ],
      "algorithmDetails": [
        {
          "name": "Research Agent Algorithm",
          "steps": [
            "Receive research topic from user",
            "Call Google Search API for popular sources",
            "Call Wikipedia API for encyclopedic summaries",
            "Call ArXiv API for academic papers",
            "Fact-check all retrieved claims via Google Fact Check API",
            "Aggregate results into research_summary object"
          ]
        },
        {
          "name": "Reporting Agent Algorithm",
          "steps": [
            "Receive research_summary from Research Agent",
            "Generate structured article via LLM with predefined template",
            "Ensure formatting matches required sections: Title, Introduction, Key Insights, Industry Impact, Future Prospects, Conclusion",
            "Return formatted article to Storage Agent"
          ]
        },
        {
          "name": "Storage Agent Algorithm",
          "steps": [
            "Serialize article and metadata",
            "Write to .md file for human readability",
            "Write to .json file for programmatic access",
            "Convert final article into PDF using WeasyPrint",
            "Return file paths to frontend"
          ]
        }
      ],
      "datasetInfo": "Nexus does not rely on a traditional dataset. Instead, its 'dataset' is dynamic web information, pulled live on each query. Sources include: Google Search (popular media and blogs), Wikipedia (encyclopedic summaries), ArXiv (peer-reviewed academic papers), and Google Fact Check API (claim verification). This ensures that content remains real-time, current, and continuously evolving.",
      "results": {
        "description": "Nexus demonstrates significant improvements in research efficiency, quality, and user experience:",
        "highlights": [
          "Productivity Impact: Reduces research time by 90%, producing structured, high-quality reports instantly",
          "Quality Impact: Articles maintain consistent structure and clarity across topics",
          "Fact-checking increases credibility and reduces misinformation",
          "User Experience Impact: Clean UI with interactive progress indicator",
          "One-click PDF export capability",
          "Average processing time: 5–8 seconds from topic to PDF",
          "Retrieves 3–10 sources per query (configurable)",
          "Consistent article structure verified manually across topics",
          "Stable output across MD, JSON, and PDF formats"
        ]
      },
      "performanceMetrics": {
        "description": "Nexus achieves production-grade performance across multiple evaluation criteria:",
        "metricsTable": [
          {"metric": "Average research time", "value": "5–8 seconds", "notes": "Topic → PDF"},
          {"metric": "Sources retrieved", "value": "3–10", "notes": "Configurable"},
          {"metric": "Article structure score", "value": "Consistent", "notes": "Verified manually across topics"},
          {"metric": "Fact-check coverage", "value": "80–100%", "notes": "Depending on topic availability"},
          {"metric": "Export formats", "value": "MD, JSON, PDF", "notes": "Stable output"},
          {"metric": "Workflow determinism", "value": "100%", "notes": "LangGraph state enforcement"}
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "Manual Research",
          "strength": "High precision from domain experts",
          "weakness": "Slow, inconsistent, cognitively demanding"
        },
        {
          "approach": "Basic Web Scrapers",
          "strength": "Fast retrieval of raw data",
          "weakness": "No synthesis or fact-checking capabilities"
        },
        {
          "approach": "LLM-only Systems",
          "strength": "Fluent summaries and natural language generation",
          "weakness": "Hallucinations, no source reliability verification"
        },
        {
          "approach": "Nexus (Hybrid)",
          "strength": "Retrieval + Fact-check + LLM synthesis + PDF export with workflow orchestration",
          "weakness": "Requires API keys for external services"
        }
      ],
      "comparativeConclusion": "Nexus uniquely combines retrieval intelligence + generative intelligence + workflow orchestration.",
      "useCases": [
        {
          "title": "Academic Research Assistance",
          "description": "Accelerates literature surveys and background research for students and researchers"
        },
        {
          "title": "News Article Drafting",
          "description": "Generates structured news articles with verified facts from multiple sources"
        },
        {
          "title": "Technical Blog Writing",
          "description": "Creates comprehensive technical blog posts with proper citations"
        },
        {
          "title": "Corporate Intelligence Reports",
          "description": "Automates market analysis and competitive intelligence gathering"
        }
      ],
      "techStack": [
        "Python",
        "Flask",
        "LangGraph",
        "React.js",
        "Tailwind CSS",
        "Framer Motion",
        "Google Search API",
        "Wikipedia API",
        "ArXiv API",
        "Google Fact Check API",
        "Google Gemini LLM",
        "WeasyPrint"
      ],
      "githubUrl": "https://github.com/ResearchAgenticSystem/Nexus",
      "articleUrl": "https://app.readytensor.ai/publications/nexus-a-three-pronged-agentic-ai-system-for-intelligent-research-and-analysis-Y06tMJMVmNjI",
      "liveUrl": "",
      "status": "active",
      "type": "independent"
    },
    {
      "slug": "parking-detection",
      "title": "High-Precision Polygonal Parking Slot Detection Using SwinMask2Former and Dynamic Gap Analysis",
      "category": "Vision",
      "categories": ["Vision"],
      "tagline": "Transformer-based polygon detection for real-time parking analytics",
      "metric": "Polygon IoU: 0.92",
      "precision": "mAP@50: 0.88",
      "tags": ["Computer Vision", "Swin Transformer", "Mask2Former", "Polygon Detection", "Instance Segmentation"],
      "description": "A high-precision, transformer-based vision system for automatic detection of parking slot polygons and real-time slot occupancy classification using a camera-only setup.",
      "overview": "This project proposes a high-precision, transformer-based vision system for automatic detection of parking slot polygons and real-time slot occupancy classification using a camera-only setup. Unlike traditional parking detection models that rely on bounding boxes or region heuristics, our framework models each parking slot as a multi-vertex polygon, allowing far more accurate geometric understanding. The core of the system is a SwinTransformer-based Mask2Former segmentation model, which provides robust polygonal masks under lighting variations, occlusions, tilted camera angles, and heterogeneous vehicle shapes. A second-stage classifier termed Dynamic Gap Analysis performs occupancy estimation by analyzing the geometric and appearance features inside each predicted slot. The system achieves production-grade accuracy, supports real-time inference, and is designed for deployment in smart parking lots, residential complexes, and urban mobility systems.",
      "systemArchitectureDiagram": {
        "src": "/projects/parking-detection/High-level Module Architecture (Logical).png",
        "caption": "End-to-end system architecture illustrating frame input, SwinMask2Former segmentation, polygon extraction, ROI mapping, Dynamic Gap Analysis, and final occupancy output.",
        "alt": "Complete system architecture pipeline diagram"
      },
      "problem": "Parking detection systems historically depend on either hardware sensors (ultrasonic, IR, inductive loops) which are expensive, maintenance-heavy, and inaccurate in multi-level or open environments, or bounding-box detectors (YOLO, SSD) that fail to accurately capture the geometric boundaries of irregular parking slots, or simple pixel-change detectors which break in real-world lighting variations, shadows, or occlusions. The main challenge addressed is: How can we accurately detect and classify occupancy of angled, irregular, partially occluded parking slots using only video input, while supporting real-time constraints and deployment scalability? The system must handle non-uniform slot sizes, day/night illumination differences, partial vehicle entry and exit, cameras placed at non-ideal overhead angles, and noise introduced by shadows, reflections, and rainfall.",
      "motivation": [
        "Real-world smart campuses and commercial lots increasingly seek camera-only parking analytics due to cost efficiency compared to per-slot hardware sensors. However, polygon-based segmentation is rarely used due to annotation cost and model complexity.",
        "The dataset uses polygon annotations (YOLO-format), capturing fine-grained outlines of each slot. This unlocks capabilities such as geometric reasoning instead of heuristic bounding box overlap, handling angled parking (30°, 45°, 60°, 90°), higher precision in crowded lots, vehicle-size independence, and robust occlusion handling by reasoning around polygon edges.",
        "Swin Transformers introduce hierarchical feature extraction and shifted-window attention, making them ideal for segmentation tasks requiring long-range spatial reasoning, such as parking slot outline detection."
      ],
      "keyMetrics": [
        {"name": "Polygon IoU", "value": "0.92"},
        {"name": "mAP@50", "value": "0.88"},
        {"name": "Occupancy Accuracy", "value": "95-97%"},
        {"name": "F1-Score", "value": "0.96"}
      ],
      "approach": "The pipeline consists of two major branches: (A) Slot Geometry Extraction: Input frame → SwinMask2Former → Binary instance masks → Contour extraction → Polygon approximation → Polygon vertices normalized and ordered → Each slot cropped into a canonical ROI. (B) Occupancy Determination: Extract geometric + visual features inside the polygon → Analyze edges and internal gaps using Dynamic Gap Analysis → Apply temporal smoothing across frames → Final binary classification: occupied / vacant. This two-step system is significantly more robust than end-to-end classification because the segmentation step ensures clean geometric outlines and the classification step uses richer spatial cues.",
      "approachSteps": [
        "Slot Geometry Extraction: Input frame processed through SwinMask2Former to generate binary instance masks",
        "Polygon Extraction: Masks converted to contours, then approximated to multi-vertex polygons with normalized and ordered vertices",
        "ROI Canonical Mapping: Each polygon warp-transformed into fixed-size 2D patch",
        "Occupancy Determination: Geometric and visual features extracted inside polygon using Dynamic Gap Analysis",
        "Temporal Integration: Median filter over sliding window eliminates flicker during vehicle movement",
        "Final Classification: Binary output (occupied/vacant) with temporal stability"
      ],  
      "architectureDescription": "The complete system is composed of: (1) Preprocessing Module for frame normalization, perspective correction, and augmentation during training; (2) SwinMask2Former Segmentation Network that outputs instance masks for each parking slot; (3) Polygon Extraction Module using OpenCV contour extraction and simplification; (4) ROI Canonical Mapping that warp-transforms each polygon into a fixed-size 2D patch; (5) Dynamic Gap Analysis analyzing edge occupancy, pixel density, structural irregularities, and shadow-resistant intensity features; (6) Temporal Integration using median filter over sliding window to eliminate flicker.",
      "architectureComponents": [
        {"name": "Preprocessing Module", "description": "Frame normalization, perspective correction when required, augmentation during training (flip, brightness shifts, random noise)"},
        {"name": "SwinMask2Former Network", "description": "Segmentation network that outputs instance masks for each parking slot from entire frame"},
        {"name": "Polygon Extraction", "description": "Uses OpenCV contour extraction, simplifies mask boundary into N vertices, generates metadata: centroid, orientation, area"},
        {"name": "ROI Canonical Mapping", "description": "Each polygon is warp-transformed into a fixed-size 2D patch for consistent analysis"},
        {"name": "Dynamic Gap Analysis", "description": "Analyzes edge occupancy, pixel density, structural irregularities, and shadow-resistant intensity features"},
        {"name": "Temporal Integration", "description": "Median filter over sliding window eliminates flicker during vehicle movement"}
      ],
      "modelArchitecture": [
        {"title": "Backbone: Swin Transformer (Tiny/Base)", "content": "Patch merging + shifted window attention with multi-scale feature hierarchies. Excellent global-local pairing for scenes with multiple overlapping polygons."},
        {"title": "Segmentation Head: Mask2Former", "content": "Query-based mask decoding with 3-level FPN integration. End-to-end trainable unified architecture for semantic + instance segmentation. Result: Pixel-perfect per-slot masks that outperform classical UNet or DeepLab models in angled/top-down scenarios."}
      ],
      "modelArchitectureDiagram1": {
        "src": "/projects/parking-detection/Fig_2.png",
        "caption": "Dual-stream feature extraction: GLCM texture pipeline and swin transformer backbone.",
        "alt": "Model architecture diagram1"
      },
      "modelArchitectureDiagram2": {
        "src": "/projects/parking-detection/Fig_3.png",
        "caption": "Hybrid feature fusion and pixel decoder for high-resolution representation learning.",
        "alt": "Model architecture diagram2"
      },
      "modelArchitectureDiagram3": {
        "src": "/projects/parking-detection/Fig_4.png",
        "caption": "Query-based transformer decoder, prediction heads, and post-processing pipeline.",
        "alt": "Model architecture diagram3"
      },
      "algorithmDetails": [
        {
          "name": "Polygon Extraction Algorithm",
          "steps": [
            "Threshold mask → binary region",
            "Largest connected component extracted",
            "Contour retrieval (cv2.findContours)",
            "Polygon simplification (cv2.approxPolyDP)",
            "Vertex count normalization",
            "Vertex ordering (clockwise sorting)"
          ]
        },
        {
          "name": "Dynamic Gap Analysis Algorithm",
          "steps": [
            "Computes gradients using Sobel filters",
            "Detects vehicle boundary pressing against slot edges",
            "Measures free-space ratios",
            "Defines adaptive thresholds based on illumination",
            "Uses morphological operations to stabilize results"
          ]
        }
      ],
      "datasetInfo": {
        "description": "The dataset includes 2400+ labeled frames with manually annotated polygon slots across daylight, dusk, and night conditions. Mixed camera positions (high angle, medium, CCTV-distance) with various slot shapes: Standard, slanted, L-shaped, compact, two-wheeler slots. Multiple vehicle categories: Cars, SUVs, vans, autos, bikes.",
        "highlights": [
          "2400+ labeled frames",
          "Polygon annotations in YOLO format",
          "Daylight, dusk, and night conditions",
          "Mixed camera positions and angles",
          "Multiple slot shapes: Standard, slanted, L-shaped, compact, two-wheeler",
          "Multiple vehicle categories: Cars, SUVs, vans, autos, bikes",
          "Augmentations: Random perspective warp, brightness shifts (−20% to +30%), Gaussian blur/noise, scale jittering"
        ]
      },
      "trainingProcess": [
        {
          "name": "Framework",
          "details": [
            "Detectron2 + PyTorch",
            "Swin-T and Swin-B variants"
          ]
        },
        {
          "name": "Hyperparameters",
          "details": [
            "Batch size: 8",
            "Epochs: ~50",
            "Optimizer: AdamW",
            "LR schedule: Warmup + cosine decay"
          ]
        },
        {
          "name": "Losses",
          "details": [
            "BCE loss for masks",
            "Dice loss",
            "Transformer query loss"
          ]
        },
        {
          "name": "Validation",
          "details": [
            "IoU sweeps",
            "Confusion matrix for occupancy classification",
            "Temporal consistency evaluation"
          ]
        }
      ],
      "results": {
        "description": "The system achieves production-grade accuracy with robust performance across various conditions.",
        "highlights": [
          "Polygon IoU: 0.92 for accurate geometric detection",
          "mAP@50: 0.88 for instance segmentation quality",
          "Vertex localization error: < 3.5 pixels mean deviation",
          "Occupancy accuracy: 95-97% using Dynamic Gap Analysis",
          "F1-score: 0.96 with nighttime consistency: ~93%",
          "GPU inference: 25-40 ms/frame, CPU inference: 200-400 ms/frame",
          "Deployment-ready for real-world video analytics"
        ]
      },
      "qualitativeResultsFigure": {
        "caption": "Figure 6. Pipeline: raw input, perspective transformation, intermediate masks/outputs, and final detection results (top row: inputs, bottom row: output variants).",
        "images": [
          {
            "src": "/projects/parking-detection/1.1.jpg",
            "alt": "Raw input image",
            "width": "60%"
          },
          {
            "src": "/projects/parking-detection/1.2.jpg",
            "alt": "Perspective transformed input",
            "width": "35%"
          },
          {
            "src": "/projects/parking-detection/1.3.jpg",
            "alt": "Intermediate mask output",
            "width": "35%"
          },
          {
            "src": "/projects/parking-detection/1.4.jpg",
            "alt": "Processing intermediate result",
            "width": "32%"
          },
          {
            "src": "/projects/parking-detection/1.5.jpg",
            "alt": "Final detection result",
            "width": "30%"
          }
        ]
      },
      "performanceMetrics": {
        "description": "Comprehensive evaluation across segmentation quality, classification accuracy, and system performance.",
        "evaluationTable": {
          "label": "Final Model Evaluation using mAP",
          "caption": "Final model evaluation metrics showing high-precision detection and segmentation performance at different IoU thresholds.",
          "columns": ["Metric", "Threshold", "Result"],
          "rows": [
            {
              "metric": "mAP50 (Main detection)",
              "threshold": "IoU = 0.50",
              "result": "98.99%"
            },
            {
              "metric": "mAP75 (Precise segmentation)",
              "threshold": "IoU = 0.75",
              "result": "95.14%"
            }
          ]
        },
        "metricsTable": [
          {"label": "Polygon IoU", "value": "0.92"},
          {"label": "mAP@50", "value": "0.88"},
          {"label": "Vertex Localization Error", "value": "< 3.5 pixels"},
          {"label": "Occupancy Accuracy", "value": "95-97%"},
          {"label": "Precision", "value": "0.96"},
          {"label": "Recall", "value": "0.96"},
          {"label": "F1-Score", "value": "0.96"},
          {"label": "Nighttime Consistency", "value": "~93%"},
          {"label": "GPU Inference", "value": "25-40 ms/frame"},
          {"label": "CPU Inference", "value": "200-400 ms/frame"}
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "YOLOv5 Bounding Box",
          "points": [
            "Accuracy: ~78%",
            "Weakness: Fails for angled slots and irregular geometries"
          ]
        },
        {
          "approach": "UNet Segmentation",
          "points": [
            "Accuracy: ~83%",
            "Weakness: Poor boundary precision for polygon vertices"
          ]
        },
        {
          "approach": "DeepLabV3+",
          "points": [
            "Accuracy: ~86%",
            "Weakness: Struggles with occlusions and overlapping regions"
          ]
        },
        {
          "approach": "SwinMask2Former (This Work)",
          "points": [
            "Accuracy: 95-97%",
            "Strength: Best geometry understanding and mask quality with robust polygon detection"
          ]
        }
      ],
      "comparisonTable": {
        "label": "Comparison of Methods with Precision and Recall",
        "caption": "Comprehensive comparison of parking detection methods showing architecture types and performance metrics across different evaluation protocols.",
        "columns": ["Method", "Architecture", "mAP@0.5 / Precision", "mAP@0.75 / Recall"],
        "rows": [
          {
            "method": "HSM2F (proposed)",
            "architecture": "Swin transformer",
            "mapPrecision": "98.99%",
            "mapRecall": "95.14%"
          },
          {
            "method": "DCNN-entrance line (Li et al. 2020)",
            "architecture": "Custom DCNN",
            "mapPrecision": "99.68% precision",
            "mapRecall": "99.41% recall"
          },
          {
            "method": "DeepPS (Zhang et al. 2018)",
            "architecture": "YOLOv2 + AlexNet",
            "mapPrecision": "99.54% precision",
            "mapRecall": "98.89% recall"
          },
          {
            "method": "DMPR-PS (Huang et al. 2019)",
            "architecture": "Directional regression DCNN",
            "mapPrecision": "99.42% precision",
            "mapRecall": "99.37% recall"
          },
          {
            "method": "YOLOv8s (car detection)",
            "architecture": "CSPDarknet",
            "mapPrecision": "95.5% mAP@0.5",
            "mapRecall": "82.8% mAP@0.5-0.95"
          },
          {
            "method": "mAlexNet CNN (Amato et al. 2016)",
            "architecture": "Compact CNN",
            "mapPrecision": "99.6% classification accuracy",
            "mapRecall": "N/A"
          },
          {
            "method": "Mask R-CNN + IoU fusion (Adi et al. 2024)",
            "architecture": "ResNet backbone",
            "mapPrecision": "98.88% (ED method)",
            "mapRecall": "N/A"
          },
          {
            "method": "LSTM/GRU forecasting (Arjona et al. 2020)",
            "architecture": "Recurrent networks",
            "mapPrecision": "0.089 RMSE (GRU)",
            "mapRecall": "Predictive model"
          }
        ]
      },
      "ablationStudies": {
        "description": "Ablation experiments explored various system components to understand their individual contributions to overall performance.",
        "findings": [
          "Effect of Polygon Simplification: Too much simplification → boundary collapse → 7% drop in IoU",
          "Effect of Removing Temporal Filtering: Flicker during entry/exit → 10-12% drop in consistency score",
          "Effect of Removing Dynamic Gap Analysis: Occupancy accuracy drops from 97% → 91%",
          "Backbone Variants: Swin-Tiny (mAP@50: 0.85, lightest, real-time), Swin-Base (mAP@50: 0.88, best accuracy), ResNet-50 (mAP@50: 0.79, worst polygon precision)"
        ]
      },
      "ablationTable": {
        "label": "TABLE 2 - Ablation Study",
        "caption": "Table 2. Ablation study showing impact of different architecture components on vehicle detection performance.",
        "columns": ["Configuration", "Architecture Components", "mAP@0.5 / mAP@0.75"],
        "rows": [
          {
            "configuration": "Full system",
            "components": "Swin transformer + GLCM Fusion + M2F (Polygon) + Dynamic slot finder",
            "performance": "98.99% / 95.14%"
          },
          {
            "configuration": "Without GLCM",
            "components": "Swin transformer + M2F (Polygon) + Dynamic slot finder",
            "performance": "96.80% / 91.20%"
          },
          {
            "configuration": "Bounding box (No polygon)",
            "components": "Swin transformer + GLCM fusion + Bbox decoder + Dynamic slot finder",
            "performance": "97.50% / 88.30%"
          },
          {
            "configuration": "Without dynamic slot finder",
            "components": "Swin transformer + GLCM fusion + M2F (Polygon) + static pre-marking",
            "performance": "99.26% / 94.72%"
          }
        ]
      },
      "useCases": [
        {"title": "Smart Parking Guidance", "description": "Real-time vacant slot detection for driver assistance systems"},
        {"title": "Automated Parking Enforcement", "description": "Detection of unauthorized parking and violation monitoring"},
        {"title": "No-Parking Zone Detection", "description": "Automated enforcement of restricted parking areas"},
        {"title": "Digital Twin Modeling", "description": "Real-time occupancy data for parking lot digital twins"},
        {"title": "Fleet Parking Analytics", "description": "Analytics for commercial fleet management and optimization"},
        {"title": "Commercial & Airport Management", "description": "Large-scale parking management for malls and airports"},
        {"title": "Residential Automation", "description": "Smart parking systems for residential complexes and societies"}
      ],
      "techStack": [
        "PyTorch",
        "Detectron2",
        "Swin Transformer",
        "Mask2Former",
        "OpenCV",
        "ONNX Runtime",
        "CVAT (polygon annotation)",
        "Python",
        "FastAPI",
        "Redis Queue",
        "Docker",
        "TensorRT",
        "NVIDIA GPU"
      ],
      "githubUrl": "https://github.com/Car-Parking-Vacancy-Detection-System/Parking-Vacancy-Detection",
      "liveUrl": "",
      "status": "active",
      "type": "research"
    },
    {
      "slug": "sanchalak",
      "title": "Sanchalak: Farmer Voice Assistant",
      "category": "Audio",
      "categories": ["Audio", "AgriTech"],
      "tagline": "Multilingual voice AI for government scheme eligibility",
      "metric": "<5s Latency",
      "tags": ["Voice AI", "Prolog", "LLM", "Multilingual", "FastAPI", "AgriTech"],
      "description": "A voice-driven AI assistant bridging the information gap between India's farmers and 50+ government welfare schemes through natural voice interaction.",
      "overview": "Sanchalak is an intelligent, multilingual, voice-driven assistant built to help Indian farmers instantly determine their eligibility for various government welfare schemes. The system offers a voice-first, regionally multilingual, and extremely low-latency (<5 seconds) conversational experience that bridges the digital divide. What makes Sanchalak uniquely powerful is its hybrid AI reasoning architecture, which combines a Prolog-based deterministic rule engine for 100% accurate and auditable eligibility decisions, a Large Language Model (Gemma) for natural, empathetic conversational responses, and seamless integration with speech-to-text (Whisper), machine translation (Azure), and text-to-speech (Azure) services. The system remembers a farmer's inputs across the session, supports highly diverse linguistic contexts, and is designed with real-world, rural deployment constraints in mind.",
      "problem": "India has over 140 million farmer households, yet millions fail to benefit from government schemes due to complex eligibility rules, scattered hard-to-understand information spread across government PDFs and websites primarily in English, digital and linguistic divide where many farmers cannot navigate official portals or read English/Hindi and only speak regional dialects, and high dependency on intermediaries who may misguide them or charge high fees. Sanchalak directly addresses all these issues by enabling farmers to get accurate eligibility results through simple voice interactions in their native language.",
      "motivation": [
        "Government farming schemes such as PM-KISAN, PMFBY, PKVY, and state-level programs aim to provide financial assistance, insurance, irrigation benefits, agricultural inputs, and subsidies. However, eligibility rules are deterministic but difficult to interpret, eligibility checking at scale requires precision not probabilistic LLM best guesses, and farmers require human-like explanation not cold technical responses.",
        "This creates a need for a hybrid model where Symbolic AI (Prolog) handles deterministic eligibility, Generative AI (LLM) handles conversational clarity and empathy, and voice-driven multimodal I/O eliminates the literacy barrier. Sanchalak is built to serve this exact requirement."
      ],
      "keyMetrics": [
        {"name": "End-to-end latency", "value": "< 5 seconds"},
        {"name": "STT accuracy (Whisper)", "value": "~95% for rural Indian accents"},
        {"name": "Translation accuracy (Azure)", "value": "≥90% for Hindi, Punjabi, Tamil"},
        {"name": "Eligibility decision accuracy", "value": "100% deterministic (Prolog)"}
      ],
      "approach": "Sanchalak follows a pipeline-based hybrid AI approach consisting of eight key steps: Voice Input is transcribed using Whisper STT, language is translated to English via Azure Translator for normalized processing, NLU entity extraction identifies key parameters (land size, irrigation, crop, insurance, category), State Manager determines if all information is collected or asks follow-up questions, Prolog Rule Engine evaluates eligibility by asserting facts and querying rules, LLM Humanization via Gemma rewrites technical results into friendly explanations, Translation and TTS convert responses back to regional language audio, and Response Playback streams audio back to the farmer in under 5 seconds.",
      "approachSteps": [
        "Voice Input → Speech-to-Text: User speaks in regional language, Whisper transcribes to text with ~95% accuracy for rural Indian accents",
        "Language Translation: Azure Translator converts regional language to English for normalized logic processing",
        "NLU Entity Extraction: System extracts key parameters (land_size, irrigation type, crop, insurance enrollment, category SC/ST/General)",
        "State Manager: Decides whether farmer has provided all necessary information, asks follow-up questions if needed",
        "Prolog Rule Engine: Facts asserted into logic engine, evaluates eligible(Scheme) and returns exact qualifying schemes",
        "LLM Humanization (Gemma): Rewrites technical eligibility result into friendly, encouraging explanation with empathy",
        "Translation + TTS: Azure services convert English → regional language → audio with natural-sounding voices",
        "Response Playback: Audio streamed back to farmer in ≤5 seconds total pipeline time"
      ],
      "architectureDiagram": {
        "src": "/projects/sanchalak/sanchalak_architecture.svg",
        "caption": "High-level architecture of Sanchalak showing the Streamlit frontend, FastAPI orchestration layer, Prolog rule engine, session database, and external AI services (Whisper, Azure Translator, Azure TTS, Gemma LLM).",
        "alt": "Sanchalak system architecture diagram"
      },
      "architecture": [
        {
          "tier": "Tier 1 - Presentation Layer",
          "description": "Streamlit handles audio recording/playback and conversation history."
        },
        {
          "tier": "Tier 2 - Application Layer",
          "description": "FastAPI orchestrates asynchronous STT/translation/LLM/TTS calls, entity extraction, session memory, and Prolog eligibility computation."
        },
        {
          "tier": "Tier 3 - Data & Knowledge Layer",
          "description": "Farmer Records Database stores session details, Scheme Rule Server maintains scheme.yaml (human-readable) and rules.pl (compiled Prolog logic)."
        }
      ],
      "architectureDescription": "Sanchalak uses a modular microservice architecture designed for scalability and maintainability. The Streamlit frontend provides a simple voice interface for farmers, while FastAPI serves as the central orchestrator managing all AI service integrations. The Prolog rule engine acts as the deterministic truth engine guaranteeing correctness, while Gemma LLM adds conversational empathy. Session state is maintained in MongoDB to enable continuity across conversations without repetition.",
      "architectureComponents": [
        {"name": "Streamlit Frontend", "description": "Handles user audio recording and playback, displays conversation history, connects to backend through REST endpoint"},
        {"name": "FastAPI Orchestrator", "description": "Central coordinator for asynchronous STT, translation, LLM, TTS calls, entity extraction, session memory management, and response packaging"},
        {"name": "Farmer Records Database (EFRDB)", "description": "Stores session-level details, ensures continuity across conversation turns"},
        {"name": "Prolog Rule Engine", "description": "Deterministic truth engine with 100% accuracy, evaluates eligibility rules, auditable and explainable"},
        {"name": "Scheme Rule Server", "description": "Maintains scheme.yaml (human-readable rules) and rules.pl (compiled Prolog logic rules)"},
        {"name": "External AI Services", "description": "Whisper STT, Azure Translator, Gemma LLM, Azure Neural TTS integrated via REST APIs"}
      ],
      "modelArchitecture": [
        {"title": "Whisper (Speech-to-Text)", "content": "Handles noisy rural environments, robust to multilingual accents, ~95% accuracy for Indian languages with ~5-8% WER"},
        {"title": "Azure Translator", "content": "Converts between any Indian language ↔ English, ensures NLU receives normalized English text, >90% accuracy with avg BLEU >90"},
        {"title": "Gemma LLM", "content": "Lightweight, fast, open-source model that reformulates Prolog output into human-friendly responses, adds empathy and politeness, ~800-1200ms latency"},
        {"title": "Azure Neural TTS", "content": "Produces natural-sounding voices in regional languages, enhances trust and usability, <1s synthesis delay"},
        {"title": "Prolog Rule Engine", "content": "Acts as the deterministic truth engine, guarantees correctness for eligibility, auditable and explainable, <20ms decision time"}
      ],
      "algorithmDetails": [
        {
          "name": "NLU Entity Extraction",
          "description": "Extracts farmer attributes from conversational text using multiple techniques.",
          "steps": [
            "Regex-based extraction for numeric information (land size, income)",
            "Semantic keyword mapping for crop/caste/irrigation attributes",
            "Confidence-based conflict resolution when multiple interpretations exist"
          ]
        },
        {
          "name": "Conversation Flow Graph (Schemabot)",
          "description": "Structured decision tree that guides the conversation step-by-step.",
          "steps": [
            "Determines which farmer attribute is currently missing",
            "Generates appropriate follow-up questions",
            "Maintains conversation context across turns"
          ]
        },
        {
          "name": "Prolog Fact Assertion & Eligibility Querying",
          "description": "Converts farmer data to logic clauses and evaluates eligibility rules.",
          "steps": [
            "Farmer data converted to Prolog facts: farmer(land_size, 3), farmer(crop, rice)",
            "Query execution: eligible(Scheme) returns list of qualifying schemes",
            "Deterministic evaluation with 100% accuracy"
          ]
        },
        {
          "name": "LLM Humanization Algorithm",
          "description": "Transforms technical eligibility results into empathetic explanations.",
          "steps": [
            "Gemma receives structured prompt with detected schemes, missing schemes with reasons, recommended actions",
            "Generates user-friendly summaries in English",
            "Azure TTS converts summaries into speech in user's regional language"
          ]
        }
      ],
      "results": {
        "description": "Sanchalak achieves exceptional performance across all system components and delivers significant user impact.",
        "highlights": [
          "End-to-end latency: < 5 seconds total pipeline time",
          "Whisper STT accuracy: ~95% for Indian languages with 5-8% WER",
          "Azure Translator quality: >90% accuracy with BLEU score >90",
          "LLM response clarity: Human-evaluated high quality",
          "Prolog eligibility decision: 100% deterministic accuracy with <20ms decision time",
          "Eliminates need for literacy and removes intermediary dependency",
          "Gives farmers agency and independence in accessing government schemes",
          "Converts complex bureaucratic rules into friendly voice guidance",
          "Modular microservice architecture can serve thousands of farmers concurrently"
        ]
      },
      "performanceMetrics": {
        "description": "Comprehensive performance evaluation across all system components showing sub-5 second end-to-end latency.",
        "metricsTable": [
          {"label": "STT (Whisper) WER", "value": "~5-8%"},
          {"label": "Translation Avg BLEU", "value": ">90"},
          {"label": "LLM Latency", "value": "~800-1200ms"},
          {"label": "Prolog Engine Decision Time", "value": "< 20ms"},
          {"label": "TTS Synthesis Delay", "value": "< 1s"},
          {"label": "Total Pipeline End-to-End", "value": "< 5s"}
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "LLM-only decision-making",
          "points": [
            "Strength: Conversational and natural",
            "Weakness: Hallucinates eligibility rules, unsafe for production use"
          ]
        },
        {
          "approach": "Rule engine only",
          "points": [
            "Accuracy: 100% deterministic",
            "Weakness: Not user-friendly, technical output difficult for farmers"
          ]
        },
        {
          "approach": "Keyword-based chatbot",
          "points": [
            "Strength: Simple implementation",
            "Weakness: Fails for complex logic, low accuracy"
          ]
        },
        {
          "approach": "Sanchalak (Hybrid)",
          "points": [
            "Accuracy: 100% deterministic logic",
            "Strength: Best of both worlds - accurate + conversational",
            "Note: More complex pipeline but production-ready"
          ]
        }
      ],
      "useCases": [
        {"title": "Government & Administrative", "description": "Farmer support helplines, public service centers, automated scheme eligibility kiosks"},
        {"title": "Agricultural Cooperatives", "description": "Advising members on subsidies and insurance programs with accurate eligibility information"},
        {"title": "NGOs & Rural Digital Literacy Campaigns", "description": "Empowering non-literate farmers with voice-based technology access"},
        {"title": "Banks & Financial Institutions", "description": "Checking eligibility for agricultural loans or credit schemes before application"}
      ],
      "techStack": [
        "Streamlit",
        "FastAPI",
        "Python",
        "SWI-Prolog",
        "YAML",
        "OpenAI Whisper",
        "Azure Translator",
        "Azure TTS",
        "Gemma LLM",
        "MongoDB",
        "Docker",
        "WebRTC"
      ],
      "githubUrl": "https://github.com/Annam-hack/Sanchalak",
      "articleUrl": "https://annam.ai/2025/08/11/sanchalak-revolutionizing-rural-welfare-access/",
      "liveUrl": "",
      "status": "active",
      "type": "internship"
    },
    {
      "slug": "luna",
      "title": "L.U.N.A.: Logical Universal Networked Assistant",
      "category": "Audio",
      "categories": ["Audio", "Vision"],
      "tagline": "AI-powered voice assistant with multimodal interaction",
      "metric": "<2s Response Time",
      "tags": ["Voice AI", "Computer Vision", "LLM", "System Automation", "PyQt5", "Multimodal AI"],
      "overview": "L.U.N.A. is an intelligent voice-controlled AI assistant that combines conversational AI, computer vision, and system automation into a unified desktop application. It enables hands-free interaction with your computer through natural voice commands, providing real-time information, automating tasks, generating images, and analyzing visual content from your screen or camera. The assistant features a modern PyQt5 interface with animated graphics and supports multi-turn conversations with context awareness. L.U.N.A. intelligently routes queries between offline processing and cloud-based AI services to balance speed, privacy, and capability.",
      "problem": "Modern productivity requires constant switching between applications, manual searches, and repetitive tasks. Users need to stop what they're doing to search for information online, manually open and close applications, type commands instead of speaking naturally, switch contexts frequently breaking workflow concentration, and lack accessible AI assistance for visual analysis tasks. Existing voice assistants are often cloud-dependent, privacy-concerning, limited to specific ecosystems, or lack deep system integration.",
      "motivation": [
        "This project was built to create a truly versatile AI companion that reduces friction in daily computing tasks through voice control, maintains privacy by processing locally when possible, adapts intelligently by choosing the right AI model for each task, provides multimodal interaction through voice, text, and vision, and demonstrates modern AI integration using state-of-the-art APIs.",
        "The motivation stemmed from wanting seamless human-computer interaction without sacrificing control or privacy, while learning to orchestrate multiple AI services into a cohesive user experience."
      ],
      "keyMetrics": [
        {"name": "Task Types", "value": "10+", "description": "Distinct capabilities including conversation, search, automation, vision analysis"},
        {"name": "App Support", "value": "100+", "description": "Supported applications and websites for automation"},
        {"name": "Response Time", "value": "<2s", "description": "Average response time for general queries"},
        {"name": "Classification Accuracy", "value": "~92%", "description": "Correct intent detection by decision-making model"},
        {"name": "Task Success Rate", "value": "~95%", "description": "For application automation commands"},
        {"name": "Voice Recognition", "value": "85-90%", "description": "Accuracy in quiet environments"}
      ],
      "approach": {
        "description": "Every user query passes through a sophisticated routing system that intelligently classifies and executes requests across multiple AI services and automation engines.",
        "steps": [
          "Intelligent Query Classification: Decision-Making Model (Cohere Command-R-Plus) analyzes intent and classifies requests into categories: General conversation, Realtime search, Task automation, Content generation, Image generation, or Vision analysis",
          "Hybrid Execution Pipeline: Based on classification, routes to specialized handlers - Chatbot Module (Groq Llama 3-70B), Search Engine with LLM summarization, Automation Engine (AppOpener + Selenium), Vision Modules (Google Gemini 2.0 Flash), or TTS/STT services",
          "Asynchronous Processing: Multiple tasks execute in parallel using asyncio, enabling commands like 'open Spotify, search for AI news, and generate an image of a sunset' to run simultaneously",
          "Context-Aware Conversations: Chat history persists in JSON format, allowing L.U.N.A. to reference previous exchanges and maintain coherent multi-turn dialogues",
          "Voice Pipeline: Microphone → Web Speech API → Language Detection → Optional Translation → Query Processing → Response Generation → Text-to-Speech Output",
          "Automation Logic: Tries AppOpener first → Falls back to predefined URL dictionary (100+ sites) → Falls back to Google search for official website, with graceful error handling"
        ]
      },
      "architecture": [
        {
          "tier": "Voice Input Layer",
          "description": "Web Speech API captures voice in headless Chrome, with optional translation to English for non-English input"
        },
        {
          "tier": "Decision-Making Module",
          "description": "Cohere Command-R-Plus analyzes raw user query + chat history to classify intent and extract structured task list"
        },
        {
          "tier": "Parallel Execution Engine",
          "description": "asyncio-based task dispatcher routes to specialized handlers: Groq LLM, Google Search, AppOpener, Gemini Vision, or Stable Diffusion"
        },
        {
          "tier": "PyQt5 GUI Frontend",
          "description": "Modern desktop interface with animated visuals, real-time status updates, and streaming response display"
        },
        {
          "tier": "Persistence Layer",
          "description": "JSON-based chat history storage enabling context retention across sessions and multi-turn dialogue coherence"
        }
      ],
      "architectureDescription": "L.U.N.A. uses a modular microservice-like architecture where a central decision-making model routes queries to specialized AI services. Voice input flows through Web Speech API to the DMM, which classifies and parallelizes execution across multiple backends. The PyQt5 frontend provides real-time feedback while tasks execute asynchronously. Chat history persistence enables contextual awareness across conversations.",
      "systemArchitectureDiagram": {
        "src": "/projects/luna/luna.png",
        "caption": "L.U.N.A. system architecture showing the complete data flow from voice input through decision-making model to parallel execution across multiple AI services and automation engines.",
        "alt": "L.U.N.A. system architecture diagram"
      },
      "modelArchitecture": [
        {
          "title": "Cohere Command-R-Plus (Decision Maker)",
          "content": "Purpose: Intent classification and task routing. Configuration: Temperature 0.7, streaming enabled. Input: Raw user query + chat history. Output: Structured task list (e.g., ['open spotify', 'realtime weather in Tokyo']). Routes queries to appropriate execution pipelines based on detected intent patterns."
        },
        {
          "title": "Groq Llama 3-70B (Conversational AI)",
          "content": "Purpose: General conversations and question answering. Configuration: 1024 max tokens, streaming responses. Context: System prompt + real-time datetime + chat history. Features: Maintains personality, avoids verbosity, English-only responses. Provides fast inference for natural dialogue."
        },
        {
          "title": "Google Gemini 2.0 Flash (Vision AI)",
          "content": "Purpose: Screen and camera frame analysis. Input: RGB image frames (PIL format). Output: Natural language descriptions of visual content. Use Cases: 'What's on my screen?', 'Analyze my camera feed'. Enables multimodal understanding of visual context."
        },
        {
          "title": "Stable Diffusion XL (Image Generation)",
          "content": "Provider: HuggingFace via Together API. Output: 4 images per prompt (JPG format). Features: Automatic prompt formatting and batch generation. Creates AI-generated artwork from natural language descriptions."
        }
      ],
      "algorithmDetails": [
        {
          "name": "Decision-Making Flow",
          "steps": [
            "User Query → DMM Classification → [Task Extraction] → Parallel Execution",
            "Example: 'Open YouTube and tell me about quantum computing' → DMM Output: ['open youtube', 'realtime quantum computing']",
            "Parallel Execution: Thread 1: OpenApp('youtube') → Launches browser; Thread 2: GoogleSearch('quantum computing') → Fetches results → LLM Summarization",
            "Response spoken via TTS while tasks complete in background"
          ]
        },
        {
          "name": "Voice Recognition Pipeline",
          "steps": [
            "Microphone → Web Speech API → Transcript → Language Detection",
            "If non-English: mtranslate → English conversion",
            "Query Modifier adds punctuation and capitalization",
            "DMM Processing for intent classification"
          ]
        },
        {
          "name": "Automation Logic",
          "steps": [
            "App Opening: Tries AppOpener first → Falls back to predefined URL dictionary (100+ sites) → Falls back to Google search for official website",
            "Multi-Command: Uses asyncio.gather() for concurrent execution of multiple tasks",
            "Error Handling: Graceful degradation with user feedback for failed operations"
          ]
        }
      ],
      "datasetInfo": {
        "description": "No traditional training dataset - this is an integration project leveraging pre-trained models.",
        "highlights": [
          "Chat History: JSON file storing conversation context (persistent across sessions)",
          "Web Search Results: Dynamic data from Google Search API (5 results per query)",
          "Real-Time Information: System datetime injected into LLM context for temporal awareness",
          "Application Database: 100+ predefined app/website mappings for automation commands",
          "Model APIs: Groq (Llama 3-70B-8192), Cohere (Command-R-Plus), Google Gemini (2.0 Flash Exp), HuggingFace (Stable Diffusion XL)"
        ]
      },
      "trainingProcess": [
        {
          "name": "Configuration & Optimization",
          "details": [
            "N/A - This project uses pre-trained foundation models via API. No model training was performed.",
            "Fine-tuned system prompts for personality and output format consistency",
            "Optimized temperature settings (0.7) for balanced creativity and accuracy",
            "Engineered prompt patterns for consistent DMM classification behavior",
            "Implemented streaming responses for improved user experience perception",
            "Designed retry logic and comprehensive error handling for API failures"
          ]
        }
      ],
      "results": {
        "description": "L.U.N.A. achieves high performance across multiple dimensions, delivering seamless multimodal interaction with strong accuracy and responsiveness.",
        "highlights": [
          "Response Latency: 1-3 seconds for general queries, 5-8 seconds for web searches with summarization",
          "Task Success Rate: ~95% for app automation, ~90% for complex multi-task queries with parallel execution",
          "Voice Recognition Accuracy: ~85-90% in quiet environments (Web Speech API dependent)",
          "Classification Accuracy: ~92% correct intent detection by decision-making model",
          "Hands-Free Operation: Enables fully voice-controlled computing without manual intervention",
          "Context Retention: Successfully maintains conversation flow across 10+ exchanges",
          "Multimodal Interaction: Seamlessly blends voice, text, vision, and automation capabilities",
          "Streaming Responses: Creates perception of instant interaction despite API latency",
          "Parallel Execution: Reduces wait times by 60% through concurrent task processing",
          "Unique Advantages: Only assistant with screen/camera AI analysis, deepest desktop application control, transparent hackable architecture, no vendor lock-in"
        ]
      },
      "useCases": [
        {"title": "Productivity Booster", "description": "'Open Slack, Zoom, and Notion, then search for project management best practices' - Instantly launches work environment and provides research while apps load"},
        {"title": "Content Creator Assistant", "description": "'Generate an image of a futuristic city, open Photoshop, and write a social media caption about AI art' - Creates visual assets, launches editing tools, and drafts accompanying text"},
        {"title": "Research Companion", "description": "'Who invented the transistor and when? Tell me more about Bell Labs' - Answers initial query from knowledge and follows up with context-aware deeper information"},
        {"title": "Accessibility Tool", "description": "Enables hands-free computer control for users with mobility limitations through voice-driven navigation and content consumption"},
        {"title": "Learning & Exploration", "description": "'What's on my screen?' (while viewing a coding tutorial) - Describes code structure and UI elements to help understand complex visual information"},
        {"title": "Entertainment", "description": "'Play Bohemian Rhapsody, dim the lights, and tell me about Queen' - Starts music playback, controls system settings, and provides contextual information"}
      ],
      "techStack": [
        "Python 3.10",
        "PyQt5",
        "Groq API (Llama 3-70B)",
        "Cohere API (Command-R-Plus)",
        "Google Gemini API (2.0 Flash)",
        "HuggingFace API (Stable Diffusion XL)",
        "Web Speech API",
        "Edge TTS",
        "asyncio",
        "Selenium WebDriver",
        "AppOpener",
        "OpenCV",
        "MSS (Screen Capture)",
        "Pillow",
        "pygame",
        "mtranslate",
        "pywhatkit",
        "googlesearch-python",
        "BeautifulSoup4",
        "dotenv"
      ],
      "githubUrl": "https://github.com/mk12002/L.U.N.A.-Logical-Universal-Networked-Assistant",
      "liveUrl": "",
      "status": "active",
      "type": "independent"
    },
    {
      "slug": "har-gcnn",
      "title": "HAR-GCNN: Robust Human Activity Recognition Using Graph Convolutional Neural Networks",
      "category": "Vision",
      "categories": ["Vision", "Healthcare"],
      "tagline": "Graph-based spatiotemporal activity classification",
      "metric": "Accuracy: 99.99%",
      "tags": ["GNN", "HAR", "Time-Series", "Sensor Data", "PAMAP2"],
      "overview": "Human Activity Recognition (HAR) is a foundational task in wearable computing, healthcare monitoring, smart fitness, and motion analytics. Traditional deep-learning solutions (CNNs, LSTMs, Transformers) struggle when sensor signals contain missing labels, irregular sampling, or high intra-class variability. This project introduces HAR-GCNN, a Graph Convolutional Neural Network architecture designed to model temporal sensor interactions as a graph instead of a flat sequence. This enables the system to learn complex, structured relationships between signals, leading to exceptional robustness even when 66% of labels are missing. The model was benchmarked on the PAMAP dataset and outperformed Meta-MAE (a transformer-based masked autoencoder), CNN, and LSTM baselines—achieving up to 99.99% accuracy.",
      "architectureDiagram": {
        "src": "/projects/har-gcnn/High-level HAR-GCNN architecture.png",
        "caption": "High-level HAR-GCNN architecture showing input graphs processed through stacked graph convolution layers and multiple CNN refinement blocks leading to final activity classification.",
        "alt": "HAR-GCNN architecture diagram"
      },
      "problem": "Wearable motion sensors generate continuous multivariate time-series data, but real-world deployments face multiple challenges: Sensors often malfunction or disconnect, producing missing labels; Motion patterns vary between users, increasing intra-class variability; Classical deep-learning models treat sequences as independent vectors, ignoring inter-feature dependencies; HAR applications require high accuracy, robustness, and graceful handling of data loss. The project addresses the core question: How can we build a human activity recognition system that remains highly accurate—even when the majority of training labels are missing?",
      "motivation": [
        "Graph Neural Networks (GNNs) enable learning over structured relationships, making them ideal for modeling sensor correlations. Instead of treating data as flat vectors, HAR-GCNN constructs a feature graph where nodes represent sensor dimensions and edges represent correlations or motion influence patterns.",
        "This allows the network to understand temporal-spatial relationships, handle irregular or corrupted signals, and learn robust graph-level representations of motion activities. Additionally, the project explores comparisons with transformer-based masked autoencoders (Meta-MAE), motivated by their ability to reconstruct missing data. The goal is to test whether graph structure learning outperforms masked reconstruction learning for HAR."
      ],
      "keyMetrics": [
        {"name": "Classification Accuracy", "value": "99.99%", "description": "Primary metric across multiple activity counts (3, 5, 10, 25 classes)"},
        {"name": "F1-Score", "value": "99.99%", "description": "Balanced precision and recall performance"},
        {"name": "Robustness", "value": "66% missing labels", "description": "Performance maintained under severe label masking"},
        {"name": "Model Size", "value": "~5000 parameters", "description": "Lightweight deployment suitable for edge devices"}
      ],
      "approach": {
        "description": "HAR-GCNN follows a five-step methodology to achieve robust activity recognition under missing-label conditions.",
        "steps": [
          "Generate Graph Representation: Sensor features (accelerometer, gyroscope, heart rate) are transformed into a feature graph where nodes encode measurements and edges encode relationships",
          "Graph Convolution Layers: HAR-GCNN applies multiple GCN layers to extract node-level and graph-level embeddings",
          "CNN Refinement Blocks: Outputs from the GCN are passed through stacked CNN layers to refine temporal features",
          "Classification Head: A softmax layer predicts the activity class",
          "Missing Label Simulation: To evaluate robustness, 66% of training labels are randomly hidden, requiring the model to learn from partially observed signals"
        ]
      },
      "architecture": [
        {
          "tier": "Graph Construction Layer",
          "description": "Converts multivariate sensor inputs into adjacency and feature matrices. Nodes represent sensor dimensions, edges represent correlations."
        },
        {
          "tier": "Graph Convolutional Network Stack",
          "description": "Multiple GCN layers that learn spatial relationships in the sensor graph, capturing inter-feature dependencies."
        },
        {
          "tier": "CNN Refinement Stack",
          "description": "Sequential convolution blocks refine temporal resolution and extract motion patterns from graph embeddings."
        },
        {
          "tier": "Classification Layer",
          "description": "Final softmax layer outputs probability distribution over activity classes."
        }
      ],
      "architectureDescription": "HAR-GCNN is a hybrid neural architecture that combines the strengths of Graph Convolutional Networks and Convolutional Neural Networks. The system transforms multivariate sensor inputs into feature graphs, applies graph convolutions to learn spatial relationships between sensors, uses CNN layers to refine temporal patterns, and outputs activity classifications. This hybrid approach excels because GCNs capture inter-sensor relationships and dependencies, while CNNs capture temporal evolution of motion patterns, leading to a superior representation that is robust to missing labels and noise.",
      "datasetInfo": "Dataset: PAMAP2 Human Activity Dataset. Contains 52 physical activities with accelerometer, gyroscope, and heart-rate readings sampled at ~100 Hz. Preprocessed into segments for 3, 5, 10, and 25 activity classification tasks. The dataset provides multivariate time-series data from wearable sensors, enabling evaluation across different activity granularities and complexity levels.",
      "trainingProcess": {
        "description": "All models were trained under identical conditions to ensure fair comparison and evaluate robustness under severe missing-label scenarios.",
        "details": [
          "Optimizer: Adam with learning rate tuned per model",
          "Batch size: Typically 32",
          "Dropout: Introduced to simulate real-world noise and prevent overfitting",
          "Missing label rate: Fixed at 66% to evaluate robustness under severe data loss",
          "Training consistency: All models trained for consistent epochs with identical hyperparameters"
        ]
      },
      "trainingPipelineDiagram": {
        "src": "/projects/har-gcnn/HAR-GCNN training pipeline.png",
        "caption": "HAR-GCNN training pipeline: feature graphs are generated from raw sensor data, passed through the GCN, and compared against target graphs to compute loss. Random masking simulates missing labels.",
        "alt": "HAR-GCNN training pipeline diagram"
      },
      "results": {
        "description": "HAR-GCNN achieved near-perfect performance even with severe missing-label conditions (66% labels masked). This demonstrates strong generalization, robust structural reasoning, and superior ability to model signal dependencies compared to sequence-based approaches.",
        "highlights": [
          "99.99% accuracy on PAMAP2 dataset across all activity subsets",
          "99.99% F1-Score maintaining balanced precision and recall",
          "Outperformed Meta-MAE transformer-based masked autoencoder",
          "Surpassed CNN baseline (99.75% accuracy) with better robustness",
          "Exceeded LSTM baseline (98.10% accuracy) significantly",
          "Maintained performance under 66% missing labels",
          "Lightweight deployment with ~5000 parameters",
          "Consistent performance across 3, 5, 10, and 25 activity classification tasks",
          "Graph reasoning proved superior to sequence-only approaches"
        ]
      },
      "harGcnnResultsTable": {
        "label": "HAR-GCNN Performance",
        "caption": "HAR-GCNN achieves near-perfect F1-scores and accuracy across all activity subsets despite 66% missing labels.",
        "columns": ["Number of Activities", "F-1 Score", "Accuracy"],
        "rows": [
          {"activities": "3", "f1Score": "0.999", "accuracy": "99.94%"},
          {"activities": "5", "f1Score": "1.000", "accuracy": "99.98%"},
          {"activities": "10", "f1Score": "1.000", "accuracy": "99.98%"},
          {"activities": "25", "f1Score": "1.000", "accuracy": "99.99%"}
        ]
      },
      "performanceMetrics": {
        "description": "HAR-GCNN was evaluated against three baseline architectures: CNN, LSTM, and Meta-MAE (masked autoencoder). All models were tested under identical conditions with 66% missing labels.",
        "cnnLstmComparisonTable": {
          "label": "CNN vs LSTM Baseline Comparison",
          "caption": "CNN and LSTM baseline comparison showing their performance degradation under missing-label conditions.",
          "columns": ["# of Activities", "CNN F-1 Score", "CNN Accuracy", "LSTM F-1 Score", "LSTM Accuracy"],
          "rows": [
            {"activities": "3", "cnnF1": "0.902", "cnnAccuracy": "90.22%", "lstmF1": "0.903", "lstmAccuracy": "90.26%"},
            {"activities": "5", "cnnF1": "0.996", "cnnAccuracy": "99.57%", "lstmF1": "0.950", "lstmAccuracy": "95.05%"},
            {"activities": "10", "cnnF1": "0.997", "cnnAccuracy": "99.75%", "lstmF1": "0.969", "lstmAccuracy": "96.87%"},
            {"activities": "25", "cnnF1": "0.997", "cnnAccuracy": "99.68%", "lstmF1": "0.981", "lstmAccuracy": "98.10%"}
          ]
        },
        "allModelsComparisonTable": {
          "label": "Model Comparison",
          "caption": "GCNN achieves the highest accuracy outperforming CNN, LSTM, and Meta-MAE.",
          "columns": ["Model", "Accuracy"],
          "rows": [
            {"model": "Meta-MAE", "accuracy": "89.35%"},
            {"model": "LSTM", "accuracy": "98.10%"},
            {"model": "CNN", "accuracy": "99.75%"},
            {"model": "GCNN", "accuracy": "99.99%"}
          ]
        }
      },
      "comparativeAnalysis": [
        {
          "approach": "CNN",
          "strength": "Fast inference, good for temporal patterns",
          "weakness": "Weak structural modeling, treats features independently"
        },
        {
          "approach": "LSTM",
          "strength": "Good temporal awareness, captures sequential dependencies",
          "weakness": "Struggles with irregular missing labels, slower training"
        },
        {
          "approach": "Meta-MAE",
          "strength": "Masked reconstruction robustness, transformer-based",
          "weakness": "Lower accuracy vs GCNN, higher computational cost"
        },
        {
          "approach": "HAR-GCNN (Proposed)",
          "strength": "Best overall accuracy, handles missing labels exceptionally, captures structural relationships",
          "weakness": "Requires graph creation overhead"
        }
      ],
      "comparativeConclusion": "HAR-GCNN proves that graph reasoning outperforms sequence-only and transformer reconstruction approaches for Human Activity Recognition.",
      "ablationStudies": {
        "description": "Ablation experiments revealed the importance of each architectural component:",
        "findings": [
          "GCN-only models lost temporal refinement → lower accuracy on sequential patterns",
          "CNN-only models failed to capture inter-feature relations → poor robustness to missing labels",
          "Removing masking simulation caused overfitting → model failed to generalize to real missing-data scenarios",
          "Both GCN + CNN components and masking-based robustness training are essential for achieving 99.99% accuracy"
        ]
      },
      "useCases": [
        {
          "title": "Smartwatch Fitness Tracking",
          "description": "Real-time activity classification for fitness apps and health monitoring wearables"
        },
        {
          "title": "Remote Physiotherapy Monitoring",
          "description": "Track patient exercise compliance and movement quality for rehabilitation programs"
        },
        {
          "title": "Elderly Fall Detection",
          "description": "Detect abnormal activities and falls in senior care facilities and home monitoring systems"
        },
        {
          "title": "Workplace Ergonomics",
          "description": "Analyze worker movements to prevent repetitive strain injuries and optimize workflows"
        },
        {
          "title": "Sports Performance Analytics",
          "description": "Detailed motion analysis for athletes to improve technique and prevent injuries"
        }
      ],
      "techStack": [
        "Python",
        "PyTorch",
        "PyTorch Geometric / DGL",
        "Graph Convolutional Networks",
        "PAMAP2 Dataset",
        "NumPy",
        "Pandas",
        "SciPy",
        "Matplotlib",
        "Seaborn",
        "ONNX",
        "TorchScript"
      ],
      "githubUrl": "https://github.com/HAR-RESEARCH/GCN_with_Transformer_for_HAR",
      "liveUrl": "",
      "status": "active",
      "type": "research"
    },
    {
      "slug": "medical-audio",
      "title": "Lightweight Foundation Model for Human Body Sound Analysis",
      "category": "Audio",
      "categories": ["Audio", "Healthcare"],
      "tagline": "Edge-deployable AI for zero-shot medical audio classification",
      "metric": ">80% Compression",
      "tags": ["Knowledge Distillation", "Edge AI", "Medical AI", "CLAP", "CLIP", "Zero-Shot Learning", "Signal Processing"],
      "overview": "This project, conducted as a Research Intern at Samsung PRISM, focused on developing a lightweight 'Student' foundation model for Human Body Sound analysis. The primary objective was to overcome the computational barriers of deploying massive foundation models (like CLAP and CLIP) on edge devices. By implementing a Multi-Teacher Knowledge Distillation framework, the project aimed to compress the 'knowledge' of large multi-modal models into a compact student model (under 50MB) capable of performing zero-shot classification of heart, lung, and bowel sounds on resource-constrained hardware. The project employed a rigorous Data-Centric AI approach, curating over 20,000+ medical audio samples and engineering a novel 3-stage preprocessing pipeline to mathematically isolate bio-acoustic signals from complex environmental noise. This work addresses the critical need for offline, real-time diagnostic tools that can run on low-power devices like digital stethoscopes or wearables, enabling accurate medical assessments in remote or resource-poor settings without requiring cloud connectivity.",
      "problem": "While foundation models like CLAP (Contrastive Language-Audio Pre-training) achieve state-of-the-art results in zero-shot audio classification, they are fundamentally unsuitable for edge deployment in healthcare. These models are aplenty but their size poses severe limitations on their ability to deploy on edge devices like digital stethoscopes or wearables. Medical audio data is scarce and typically contaminated with non-stationary noise (friction, ambient speech, machine hums), making it difficult to train robust small models from scratch. Furthermore, large models require significant compute resources, causing unacceptable inference latency for real-time diagnostic tools where immediate clinical decisions are critical. The challenge is threefold: achieving extreme model compression while maintaining accuracy fidelity, handling severely limited and noisy training data, and preserving the zero-shot flexibility that makes foundation models valuable in healthcare settings where new conditions constantly emerge.",
      "motivation": [
        "There is a critical need for Edge AI in Healthcare—offline, real-time diagnostic tools that can run on low-power chips to assist clinicians in remote or resource-poor settings where cloud connectivity is unreliable or unavailable. Unlike traditional classifiers fixed to specific classes (e.g., only 'pneumonia'), foundation models offer zero-shot capabilities that can detect new conditions (e.g., 'bowel obstruction') simply by changing the text prompt, without retraining the model.",
        "Recent literature (Yang et al., 2023) demonstrated that multi-teacher distillation could significantly compress speech models while retaining performance, motivating the application of this technique to bio-acoustics. The motivation stemmed from a desire to democratize access to AI-powered diagnostics by making sophisticated models deployable on affordable, portable devices that community health workers can use in the field without requiring extensive technical expertise or infrastructure."
      ],
      "keyMetrics": [
        {
          "name": "Compression Ratio",
          "value": ">80%",
          "description": "Size reduction compared to Teacher model while maintaining functionality"
        },
        {
          "name": "Model Size",
          "value": "<50MB",
          "description": "Total size combining both Text and Audio Encoders for edge deployment"
        },
        {
          "name": "Accuracy Fidelity",
          "value": "<1% Gap",
          "description": "Performance difference between Student and Teacher models"
        },
        {
          "name": "Zero-Shot Capability",
          "value": "Retained",
          "description": "Student model maintains Teacher's ability to classify unseen classes"
        },
        {
          "name": "Dataset Volume",
          "value": "20,000+",
          "description": "Curated medical audio samples from diverse open-source datasets"
        },
        {
          "name": "Sound Classes",
          "value": "10+",
          "description": "Coverage including heartbeats, murmurs, wheezing, crackles, bowel sounds"
        }
      ],
      "approach": {
        "description": "The solution employed a Data-Centric AI approach combined with Multi-Teacher Knowledge Distillation. Rather than training a small model from scratch on limited medical data, the project leveraged the knowledge already encoded in large pre-trained foundation models (CLAP and CLIP). The strategy involved curating a high-quality dataset from disparate sources, applying advanced signal processing to clean noisy recordings, validating that the Teacher models could effectively cluster the medical sounds, and then distilling their learned representations into a compact Student encoder optimized for resource-constrained devices.",
        "steps": [
          "Data Curation & Aggregation: Collected over 20,000+ medical audio samples from disparate open-source datasets (PhysioNet, AudioSet, VGGSound, ICBHI) covering diverse body sounds and clinical conditions.",
          "Advanced Signal Processing: Engineered a novel 3-stage preprocessing pipeline (Wavelet Denoising → Spectral Gating → Bandpass Filtering) to mathematically isolate bio-acoustic signals from complex environmental noise without distorting diagnostic features.",
          "Manifold Analysis & Validation: Utilized t-SNE visualization to analyze the embedding space of Teacher models (CLAP/CLIP), confirming their ability to semantically cluster the curated medical data before proceeding with distillation.",
          "Multi-Teacher Knowledge Distillation: Designed a framework where a lightweight Student encoder is trained to minimize KL-divergence between its outputs and the probability distributions of frozen Teacher models, effectively compressing their knowledge.",
          "Zero-Shot Evaluation: Validated the Student model's retained ability to perform zero-shot inference on unseen medical sound classes by changing text prompts without model retraining.",
          "Deployment Optimization: Ensured the final Student model meets strict KPIs (<50MB size, <1% accuracy gap) for deployment on edge devices like digital stethoscopes and mobile health applications."
        ]
      },
      "architecture": [
        {
          "tier": "Ingestion Module",
          "description": "Standardizes diverse audio formats (.wav, .mp3, .flac) and aligns annotations from various sources (PhysioNet, ICBHI, AudioSet) into a unified schema for consistent processing."
        },
        {
          "tier": "Preprocessing Engine",
          "description": "A sequential 3-stage filtering chain that sanitizes raw audio into high-fidelity waveforms: Wavelet Denoising (sym4, Level 6) → Spectral Gating (torchgate) → Butterworth Bandpass Filter (20-1000Hz)."
        },
        {
          "tier": "Distillation Engine",
          "description": "Multi-Teacher framework where frozen CLAP/CLIP models act as 'knowledge bases' and a lightweight Student encoder (compressed CNN/Transformer) is optimized via KL-divergence loss to mimic Teacher representations."
        },
        {
          "tier": "Evaluation Module",
          "description": "Automated validation scripts for checking model size (MB), compression ratio, zero-shot accuracy metrics, and embedding quality via t-SNE visualization."
        },
        {
          "tier": "Deployment Pipeline",
          "description": "Model quantization and optimization for inference on resource-constrained hardware (mobile CPUs, ARM processors) with real-time latency requirements."
        }
      ],
      "architectureDescription": "The system is a modular, reproducible research pipeline designed for edge AI deployment. It follows a data-centric approach where high-quality data curation and preprocessing are prioritized over architectural complexity. The Ingestion Module handles the heterogeneity of medical audio datasets, normalizing formats and annotations. The Preprocessing Engine applies domain-specific signal processing techniques tailored to bio-acoustic characteristics, removing noise while preserving diagnostic information. The core Distillation Engine implements a multi-teacher learning paradigm where the Student model learns from both CLAP (audio-language foundation model) and CLIP (vision-language model applied to spectrograms), capturing complementary knowledge representations. The Evaluation Module ensures the compressed model meets clinical deployment standards, and the Deployment Pipeline handles model optimization for edge hardware constraints.",
      "modelArchitecture": [
        {
          "title": "Teacher: CLAP (Contrastive Language-Audio Pre-training)",
          "content": "CLAP consists of two large-scale encoders working in tandem: an Audio Encoder (Transformer/CNN) that processes Mel-spectrograms, and a Text Encoder (BERT-like transformer) that processes natural language descriptions. The model learns a joint embedding space where semantically similar audio and text vectors are pulled together via contrastive loss. For example, the audio of a heart murmur and the text 'abnormal systolic heart sound' would have high cosine similarity in this shared space. This enables zero-shot classification—given a new audio sample, the model encodes it and compares it against text descriptions of various conditions without requiring task-specific fine-tuning."
        },
        {
          "title": "Teacher: CLIP (Contrastive Language-Image Pre-training)",
          "content": "While CLAP is audio-native, CLIP was leveraged by treating audio spectrograms as images, effectively adding a 'visual teacher' to the distillation process. CLIP's Image Encoder (Vision Transformer or ResNet) processes 2D spectrogram representations, while its Text Encoder handles condition descriptions. By using both CLAP and CLIP as teachers, the Student model learns from complementary perspectives: CLAP's direct audio understanding and CLIP's visual pattern recognition of spectrograms. This multi-teacher approach helps the Student generalize better across diverse medical sounds."
        },
        {
          "title": "Student: Lightweight Encoder (<50MB)",
          "content": "The Student model is a compressed encoder architecture (e.g., pruned CNN, efficient Transformer variant like MobileViT) designed specifically for edge deployment. It takes Mel-spectrograms as input and outputs embeddings in the same dimensionality as the Teachers (typically 256D or 512D). During distillation training, the Student's embeddings are compared against the Teachers' embeddings for the same audio samples, and the network weights are updated to minimize the difference. The goal is to capture the Teachers' semantic understanding in a fraction of the parameters, enabling real-time inference on devices with limited memory and compute."
        }
      ],
      "algorithmDetails": [
        {
          "name": "3-Stage Preprocessing Pipeline",
          "description": "A rigorous signal processing chain developed to handle the specific noise profiles of medical audio recordings:",
          "steps": [
            "Stage 1 - Wavelet Denoising: Uses Discrete Wavelet Transform (DWT) with sym4 (Symlet 4) wavelet at Level 6 decomposition with Medium Soft Thresholding. The sym4 wavelet is nearly symmetrical, preventing phase distortion that could alter the temporal shape of heart sounds or respiratory patterns, while effectively removing non-stationary white noise from contact friction or patient movement.",
            "Stage 2 - Spectral Gating: Applies torchgate library to remove persistent, stationary background noise (machine hums, HVAC systems, distant conversations). The algorithm computes a noise profile from silent segments and gates (suppresses) frequency bins below a learned threshold, cleaning the spectrogram while preserving the signal's spectral structure.",
            "Stage 3 - Bandpass Filtering: Implements a Butterworth Bandpass Filter with 20Hz low-cut and 1000Hz high-cut. The Butterworth design is 'maximally flat' in the passband, meaning it does not introduce amplitude distortion. This range (20-1000Hz) specifically isolates clinical body sounds—Heart sounds: 20-150Hz, Lung sounds: 100-1000Hz—while rejecting low-frequency motion artifacts (<20Hz) and high-frequency speech interference (>1000Hz)."
          ]
        },
        {
          "name": "Multi-Teacher Knowledge Distillation",
          "description": "Training methodology for compressing multiple large Teacher models into a single compact Student:",
          "steps": [
            "Freeze Teacher Parameters: Both CLAP and CLIP models are loaded with pre-trained weights and set to evaluation mode—no gradient updates occur to their parameters during distillation.",
            "Forward Pass Through Teachers: For each audio sample in the training batch, generate embeddings from both CLAP's Audio Encoder and CLIP's Image Encoder (treating the Mel-spectrogram as an image).",
            "Student Prediction: The lightweight Student encoder processes the same audio and produces its own embedding vector.",
            "Compute Distillation Loss: Calculate KL-divergence between the Student's output distribution and the averaged distribution of the Teachers. This encourages the Student to mimic the Teachers' semantic representations rather than learning from raw labels.",
            "Backpropagation: Update only the Student's weights to minimize the distillation loss, iteratively refining its ability to replicate the Teachers' understanding within a compressed parameter space."
          ]
        },
        {
          "name": "Zero-Shot Inference",
          "description": "Runtime classification process without task-specific training:",
          "steps": [
            "Encode Audio: The Student model processes the input audio sample (after preprocessing) and outputs an embedding vector.",
            "Encode Text Prompts: For each candidate medical condition (e.g., 'heart murmur', 'wheezing', 'bowel obstruction'), encode the text description using the Text Encoder.",
            "Compute Similarity: Calculate cosine similarity between the audio embedding and each text embedding.",
            "Predict Class: The condition with the highest similarity score is selected as the classification result—no model retraining needed to add new conditions."
          ]
        }
      ],
      "datasetInfo": {
        "description": "The dataset comprises over 20,000 annotated medical audio samples curated from multiple disparate open-source repositories. This diversity ensures the model can generalize across different recording equipment, patient demographics, and clinical environments. Each sample was subjected to the 3-stage preprocessing pipeline to ensure consistent quality before use in training and evaluation.",
        "highlights": [
          "PhysioNet / CinC Challenge 2016: High-quality heart sound recordings (Normal vs. Abnormal) collected from multiple clinical sites with standardized annotation protocols.",
          "ICBHI 2017 Respiratory Sound Database: Comprehensive collection of lung sounds including Wheezes, Crackles, and Normal breathing from patients with various respiratory conditions.",
          "AudioSet: Google's large-scale audio event dataset, filtered for bio-acoustic events including Bowel sounds, Coughs, and other body-related sounds.",
          "VGGSound: Audio-visual dataset providing additional diverse body sound samples with temporal alignment for multimodal learning.",
          "Class Diversity: Dataset spans 10+ distinct classes including Heartbeats, Heart Murmurs, Wheezing, Crackles (fine/coarse), Rhonchi, Normal Lung Sounds, Bowel Sounds, and various background artifacts for robust noise handling.",
          "Annotation Quality: Each sample includes metadata on recording device, patient condition, clinical context, and expert annotations for supervised validation tasks."
        ]
      },
      "trainingProcess": [
        {
          "name": "Data Preparation & Validation",
          "details": [
            "Preprocessing Pipeline: Raw audio samples were passed through the 3-stage pipeline (Wavelet → Gating → Bandpass), normalized to consistent amplitude, converted to 128-bin Mel-spectrograms with 2048 FFT size, and serialized into Apache Parquet files for efficient I/O during training.",
            "Embedding Analysis: Before beginning distillation, the entire dataset was fed through the frozen CLAP Teacher to generate 256-dimensional embeddings. t-SNE dimensionality reduction was applied to visualize these embeddings in 2D space.",
            "Manifold Validation: t-SNE plots confirmed that the Teacher successfully clustered semantically similar medical sounds—for example, Heart sounds formed distinct clusters separate from Lung sounds, and pathological sounds (Wheezes, Murmurs) separated from Normal sounds. This validation step ensured the Teachers had sufficient domain knowledge to transfer to the Student.",
            "Data Augmentation: Applied audio augmentation techniques (time-stretching, pitch-shifting, adding controlled noise) to increase dataset diversity and improve Student robustness to recording variations.",
            "Train/Val/Test Split: 70% training, 15% validation for hyperparameter tuning, 15% held-out test set for final zero-shot evaluation on completely unseen samples."
          ]
        },
        {
          "name": "Distillation Training Loop",
          "details": [
            "Batch Processing: Mini-batches of preprocessed audio samples were loaded, with corresponding embeddings pre-computed from both CLAP and CLIP Teachers to avoid redundant forward passes.",
            "Student Forward Pass: The lightweight Student encoder processed each batch, generating its own embedding vectors in the same dimensionality as the Teachers.",
            "Loss Computation: Multi-teacher distillation loss calculated as weighted KL-divergence: L = α·KL(Student || CLAP) + β·KL(Student || CLIP), where α and β are tuning weights (typically α=0.6, β=0.4 to emphasize audio-native CLAP).",
            "Optimization: Adam optimizer with learning rate 1e-4, warm-up schedule for first 5 epochs, cosine annealing to 1e-6 over 50 epochs. Gradient clipping at norm 1.0 to stabilize training.",
            "Regularization: Applied dropout (p=0.3) in Student architecture and early stopping based on validation loss to prevent overfitting to the training distribution.",
            "Monitoring: Tracked compression ratio (Student size / Teacher size), embedding alignment (cosine similarity between Student and Teacher outputs), and zero-shot accuracy on validation set throughout training."
          ]
        }
      ],
      "results": {
        "description": "The project successfully demonstrated that multi-teacher knowledge distillation can compress large foundation models for edge deployment while preserving their zero-shot capabilities. The custom preprocessing pipeline effectively isolated diagnostic signals from noisy recordings, and t-SNE analysis validated that the Teacher models could semantically cluster diverse medical sounds before distillation commenced.",
        "highlights": [
          "Compression Achievement: Successfully reduced model size by >80% compared to the original CLAP Teacher while maintaining target accuracy fidelity (<1% performance gap).",
          "Zero-Shot Validation: The Student model retained the Teacher's ability to classify previously unseen medical sound classes simply by changing text prompts, without requiring model retraining.",
          "Preprocessing Efficacy: Comparative spectrogram analysis demonstrated that the 3-stage pipeline (Wavelet → Gating → Bandpass) successfully removed broadband and stationary noise without damaging diagnostic signal components critical for clinical decision-making.",
          "Embedding Quality: t-SNE visualization of CLAP embeddings on the curated dataset showed clear semantic clustering—Heart sounds, Lung sounds, and Bowel sounds formed distinct regions, with pathological conditions (Murmurs, Wheezes) separating from Normal patterns.",
          "Edge Deployability: Final Student model achieved <50MB total size (Audio + Text encoders), making it suitable for deployment on resource-constrained devices like digital stethoscopes, smartphones, and wearable health monitors.",
          "Inference Latency: Student model achieves <200ms inference time on mobile CPUs, enabling real-time diagnostic feedback for clinicians without cloud connectivity requirements.",
          "Deliverables: Successfully delivered the complete dataset annotation/preprocessing pipeline, trained model checkpoints, embedding visualization tools, and deployment-ready Student model to the Samsung PRISM research team.",
          "Clinical Utility: The model's zero-shot capability allows it to adapt to new diagnostic scenarios (e.g., detecting COVID-19 lung sounds) without collecting new training data or retraining—only text prompt updates needed.",
          "Noise Robustness: Validated performance on intentionally corrupted test samples (added ambient speech, machinery noise) to confirm the preprocessing pipeline's effectiveness in real-world clinical environments.",
          "Scalability: The modular pipeline design enables easy extension to additional body sound modalities (e.g., cough analysis, snoring patterns) by simply adding new annotated samples to the training corpus."
        ]
      },
      "useCases": [
        {
          "title": "Digital Stethoscopes",
          "description": "Enabling analog stethoscopes to digitally analyze heart, lung, and bowel sounds and flag abnormalities (like murmurs, wheezes, or obstructions) in real-time. Clinicians receive immediate AI-powered diagnostic suggestions while conducting physical examinations, improving diagnostic accuracy especially for less experienced practitioners."
        },
        {
          "title": "Rural Tele-Health",
          "description": "Providing community health workers in remote areas with a smartphone app that can 'listen' to a patient's chest or abdomen and provide an initial triage assessment offline. The zero-shot capability allows the model to detect conditions not explicitly trained on, adapting to local disease profiles without requiring cloud connectivity or data scientists on-site."
        },
        {
          "title": "Wearable Continuous Monitoring",
          "description": "Embedding the lightweight Student model into wearable devices for continuous, low-power monitoring of post-operative patients or chronic disease management. The system can detect early signs of respiratory distress, cardiac arrhythmias, or bowel obstruction and alert healthcare providers before conditions deteriorate."
        },
        {
          "title": "Medical Education & Training",
          "description": "Medical students and nursing trainees can use the system to practice auscultation skills. The AI provides immediate feedback on what body sounds they are hearing, reinforcing learning and building diagnostic confidence before encountering real patients."
        },
        {
          "title": "Emergency Triage",
          "description": "Paramedics and emergency responders can use portable devices with the embedded model to quickly assess patient conditions in the field. The fast inference (<200ms) enables rapid triage decisions, prioritizing patients with critical cardiopulmonary issues for immediate transport."
        },
        {
          "title": "Veterinary Medicine",
          "description": "The same framework can be adapted for animal health monitoring, particularly in livestock management or companion animal care where traditional diagnostic infrastructure is limited. Zero-shot learning allows veterinarians to detect abnormal sounds across different species without species-specific training data."
        }
      ],
      "techStack": [
        "Python 3.9+",
        "PyTorch",
        "CLAP (Contrastive Language-Audio Pre-training)",
        "CLIP (Contrastive Language-Image Pre-training)",
        "AudioCLIP",
        "PyWavelets (Discrete Wavelet Transform)",
        "scipy.signal (Butterworth Filters)",
        "torchgate (Spectral Gating)",
        "librosa (Audio Processing)",
        "torchaudio",
        "Apache Parquet (Data Storage)",
        "Matplotlib",
        "Seaborn",
        "scikit-learn (t-SNE)",
        "NumPy",
        "Pandas",
        "HuggingFace Transformers",
        "ONNX Runtime (Deployment)",
        "TensorRT (Optimization)"
      ],
      "liveUrl": "",
      "status": "active",
      "type": "internship"
    },
    {
      "slug": "agricure",
      "title": "AgriCure: A Lightweight MobileNet-Based Disease Classification System for Tomato Plants",
      "category": "Vision",
      "categories": ["Vision", "AgriTech"],
      "tagline": "MobileNet-based disease detection with AWS S3 integration",
      "metric": "90%+ Accuracy",
      "tags": ["AgriTech", "MobileNet", "Deep Learning", "AWS S3", "Disease Detection", "Transfer Learning"],
      "overview": "AgriCure is an end-to-end plant disease detection system designed to assist farmers, agricultural officers, and crop-monitoring services by enabling real-time diagnosis of tomato leaf diseases using a lightweight deep learning model deployable on low-resource devices. The project leverages MobileNet, a highly efficient CNN architecture optimized for mobile deployment, enabling on-field disease identification even in areas with limited computational infrastructure. Farmers can simply upload an image of a tomato leaf, and the system automatically classifies it into one of the target disease categories while simultaneously storing images in a structured cloud repository for downstream analysis. The solution integrates lightweight deep learning (MobileNet) for disease classification, AWS S3 for scalable image storage and dataset management, data preprocessing pipeline for image cleaning, augmentation, and normalization, and a production-ready workflow suitable for smartphone applications and automated monitoring systems.",
      "systemArchitectureDiagram": {
        "src": "/projects/agricure/Block diagram.png",
        "caption": "Block diagram depicting flow of use of Amazon S3: Images are captured, uploaded to S3, processed by MobileNet, and results are returned to users while maintaining organized cloud storage for continuous retraining.",
        "alt": "AgriCure S3 workflow and system architecture diagram"
      },
      "problem": "Tomato crops are highly susceptible to fungal, viral, and bacterial diseases that can cause devastating yield losses. Traditionally, disease diagnosis relies on manual visual inspection, availability of trained agronomists, and physical laboratory testing. These methods are slow, inconsistent, and inaccessible for most small-scale farmers. The core challenge AgriCure addresses is: How can we build a fast, accurate, lightweight, and scalable disease detection system deployable on smartphones and web interfaces to assist millions of farmers in real time? Key requirements include high accuracy despite limited data, a model small enough for mobile devices, ability to function even with low network bandwidth, a clean dataset pipeline to maintain high generalization, and scalable cloud storage for images and datasets. AgriCure delivers all of these through a carefully engineered MobileNet model and an AWS-integrated workflow.",
      "motivation": [
        "Plant diseases remain one of the primary contributors to agricultural losses worldwide. Early detection dramatically reduces pesticide misuse, crop failures, and economic losses. However, India's rural regions lack expert agronomists, farmers rely on guesswork or outdated knowledge, and disease progression is rapid and often unnoticed.",
        "Deep learning has demonstrated exceptional potential in image classification, yet traditional models such as VGG16 or ResNet are too large for field deployment. MobileNet, with its depthwise separable convolutions, offers 10–20x fewer parameters, comparable accuracy to heavy CNNs, and fast inference on smartphones.",
        "AgriCure builds on these strengths, combining them with cloud-based data storage (S3) to create a full disease-diagnosis pipeline suitable for large-scale deployment."
      ],
      "keyMetrics": [
        {"name": "Classification Accuracy", "value": ">90%", "description": "High accuracy across multiple tomato disease classes"},
        {"name": "Model Size", "value": "~4-10 MB", "description": "Lightweight enough for mobile deployment"},
        {"name": "Inference Latency", "value": "<150ms", "description": "Real-time disease classification on mobile devices"},
        {"name": "Parameter Reduction", "value": "10-20x", "description": "Compared to VGG16/ResNet architectures"}
      ],
      "approach": {
        "description": "The AgriCure methodology follows a systematic six-step pipeline designed to enable accurate, fast, and scalable disease detection for real-world agricultural deployment.",
        "steps": [
          "Data Acquisition: Tomato leaf images are collected from open datasets and validated repositories, uploaded to an AWS S3 bucket that categorizes them by disease class, timestamp, and user/device source",
          "Data Preprocessing: Steps include resizing images (224×224), data augmentation (flipping, rotation, zoom), normalization, noise removal, and train–val–test splitting",
          "Model Selection: MobileNet is selected due to small model footprint, high accuracy on limited datasets, and optimized computations for ARM devices",
          "Model Training: MobileNet is fine-tuned to classify tomato diseases using Adam optimizer, learning rate schedules, and appropriate batch sizes",
          "Testing & Validation: Focus areas include inter-class confusion, class imbalance handling, and sensitivity to lighting, blur, and background noise",
          "Deployment Workflow: Images are uploaded → stored in S3 → processed by the model → results returned to user"
        ]
      },
      "architecture": [
        {
          "tier": "Client Device (Mobile/Web App)",
          "description": "Captures leaf images and sends them to the backend/S3 for processing"
        },
        {
          "tier": "AWS S3 Storage Layer",
          "description": "Stores raw images, organizes data for training updates, enables asynchronous batch processing"
        },
        {
          "tier": "Backend Deep Learning Service",
          "description": "Loads MobileNet model, preprocesses incoming images, generates predictions"
        },
        {
          "tier": "Analytics Layer",
          "description": "Tracks accuracy, monitors prediction patterns, supports retraining lifecycle"
        }
      ],
      "architectureDescription": "AgriCure follows a cloud-assisted hybrid architecture that enables scalability and easy integration with future crop-diagnosis extensions. The modular design separates concerns: client devices handle image capture, S3 manages storage and organization, the backend service runs the deep learning model, and the analytics layer provides insights for continuous improvement.",
      "modelArchitecture": [
        {
          "title": "Depthwise Separable Convolutions",
          "content": "Breaks down standard convolution into depthwise convolution (filters per channel) and pointwise convolution (1×1 linear combination). This reduces computational cost, model size, and memory footprint by 10-20x compared to traditional CNNs."
        },
        {
          "title": "Linear Bottlenecks",
          "content": "Used to compress channels before expanding them, improving gradient flow and enabling efficient feature propagation through the network."
        },
        {
          "title": "Inverted Residuals",
          "content": "Layers expand then compress instead of compress → expand. This improves feature reuse and allows high-level abstractions in lightweight networks, crucial for mobile deployment."
        },
        {
          "title": "Efficient Feature Extractor",
          "content": "Designed specifically for mobile GPUs, edge devices, and browser inference (TensorFlow.js). MobileNet in AgriCure is fine-tuned using transfer learning to adapt from ImageNet to tomato disease classification."
        }
      ],
      "modelArchitectureDiagram": {
        "src": "/projects/agricure/Architecture of MobileNet.png",
        "caption": "Architecture of MobileNet showing depthwise separable convolutions, inverted residual blocks, and linear bottlenecks that enable efficient mobile deployment.",
        "alt": "MobileNet architecture diagram with depthwise separable convolutions"
      },
      "algorithmDetails": [
        {
          "name": "Input Pipeline",
          "steps": [
            "Fetch images from S3 using unique keys",
            "Apply preprocessing transforms (resize, normalize)",
            "Feed normalized image tensors into MobileNet"
          ]
        },
        {
          "name": "Forward Pass",
          "steps": [
            "MobileNet extracts spatial features using depthwise separable convolutions",
            "Features pass through inverted residual bottleneck layers",
            "Final logits represent disease class probabilities"
          ]
        },
        {
          "name": "Prediction & Post-Processing",
          "steps": [
            "Model outputs disease classes: Healthy / Early Blight / Leaf Mold / Septoria / Target Spot / Mosaic Virus",
            "Convert logits → softmax probabilities",
            "Apply confidence thresholding",
            "Package results for user interface"
          ]
        },
        {
          "name": "Loss & Optimization",
          "steps": [
            "Loss function: Categorical Cross Entropy",
            "Optimizer: Adam with learning rate warm-ups",
            "Training strategy: Feature extraction → gradual unfreezing → full fine-tuning"
          ]
        }
      ],
      "datasetInfo": {
        "description": "The dataset consists of labeled tomato leaf images in multiple disease classes, sourced from expert-curated agricultural disease repositories and public datasets contributed by agricultural research institutions.",
        "highlights": [
          "Diverse lighting conditions (field, greenhouse, indoor)",
          "Mixed backgrounds with variability in leaf orientation",
          "Multiple disease severity levels",
          "Dataset split: Training (~70%), Validation (~15%), Testing (~15%)",
          "Preprocessing: Random rotations, color jitter, zoom transformations",
          "Noise reduction and background adjustment techniques applied",
          "Disease classes: Healthy, Early Blight, Leaf Mold, Septoria, Target Spot, Mosaic Virus"
        ]
      },
      "trainingProcess": [
        {
          "name": "Stage 1: Feature Extraction",
          "details": [
            "MobileNet loaded with pretrained ImageNet weights",
            "Only final classification layers unfrozen",
            "Quick convergence on disease-specific features"
          ]
        },
        {
          "name": "Stage 2: Fine-Tuning",
          "details": [
            "Unfreeze deeper convolutional blocks",
            "Lower learning rate for stability",
            "Gradual unfreezing strategy to prevent catastrophic forgetting"
          ]
        },
        {
          "name": "Stage 3: Evaluation",
          "details": [
            "Monitor accuracy and loss curves",
            "Adjust hyperparameters based on validation performance",
            "Early stopping based on validation loss plateau"
          ]
        },
        {
          "name": "Stage 4: Export & Deployment",
          "details": [
            "Convert trained model to ONNX / TensorFlow Lite format",
            "Optimize for edge-device inference (quantization, pruning)",
            "Package in backend server or mobile application"
          ]
        }
      ],
      "results": {
        "description": "AgriCure demonstrates strong performance across multiple evaluation criteria, making it suitable for real-world agricultural deployment.",
        "highlights": [
          "Classification accuracy exceeding 90% across all disease classes",
          "High accuracy in differentiating visually similar leaf diseases",
          "Robustness to real-world lighting variance and background noise",
          "Fast inference (<150ms) suitable for real-time mobile use",
          "Minimal overfitting due to comprehensive data augmentation",
          "Strong generalization to real field images outside training distribution",
          "Scalable cloud-backed storage enables continuous retraining and improvement",
          "Enables farmers to detect diseases early, reducing pesticide overuse",
          "Reduces dependency on offline experts and physical laboratory testing",
          "Supports government smart-agriculture initiatives and extends to multiple crops"
        ]
      },
      "performanceMetrics": {
        "description": "AgriCure achieves production-grade performance with high accuracy, fast inference, and lightweight deployment characteristics.",
        "metricsTable": [
          {"metric": "Classification Accuracy", "value": ">90%", "notes": "High accuracy across multiple tomato disease classes"},
          {"metric": "Precision (Macro Avg)", "value": "~0.88-0.92", "notes": "Balanced performance across all disease categories"},
          {"metric": "Recall (Macro Avg)", "value": "~0.87-0.91", "notes": "Strong detection rate for all disease types"},
          {"metric": "F1-Score (Macro Avg)", "value": "~0.87-0.91", "notes": "Balanced precision-recall tradeoff"},
          {"metric": "Model Size", "value": "4-10 MB", "notes": "Lightweight for mobile deployment"},
          {"metric": "Inference Latency (Mobile)", "value": "<150ms", "notes": "Real-time classification on smartphones"},
          {"metric": "Training Convergence", "value": "20-30 epochs", "notes": "Fast training with transfer learning"},
          {"metric": "S3 Upload Latency", "value": "<500ms", "notes": "Quick cloud storage integration"},
          {"metric": "Storage Cost per 1000 Images", "value": "~$0.02-0.05", "notes": "Scalable and cost-effective"}
        ]
      },
      "comparisonTable": {
        "label": "Model Architecture Comparison",
        "caption": "Comprehensive comparison of traditional CNN architectures vs. MobileNet for agricultural disease detection, showing AgriCure's efficiency advantages across size, speed, deployment capability, and accuracy.",
        "columns": ["Model", "Model Size", "Inference Speed", "Mobile Deployment", "Accuracy", "Key Characteristics"],
        "rows": [
          {
            "model": "VGG16",
            "modelSize": "Very large (>500MB)",
            "inferenceSpeed": "Slow",
            "mobileDeployment": "Not suitable",
            "accuracy": "High",
            "keyCharacteristics": "Impractical for field use despite high accuracy"
          },
          {
            "model": "ResNet50",
            "modelSize": "Large (~100MB)",
            "inferenceSpeed": "Moderate",
            "mobileDeployment": "Borderline",
            "accuracy": "High",
            "keyCharacteristics": "Resource-intensive, borderline for mobile devices"
          },
          {
            "model": "MobileNet (AgriCure)",
            "modelSize": "Very small (4-10MB)",
            "inferenceSpeed": "Fast (<150ms)",
            "mobileDeployment": "Excellent",
            "accuracy": "High (>90%)",
            "keyCharacteristics": "10-20x parameter reduction, ideal for edge devices"
          }
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "VGG16",
          "strength": "High classification accuracy on large datasets",
          "weakness": "Very large model size (>500MB) makes it impractical for mobile deployment"
        },
        {
          "approach": "ResNet50",
          "strength": "Good balance of accuracy and residual connections for deep networks",
          "weakness": "Large model size (~100MB) and moderate inference speed limit mobile usability"
        },
        {
          "approach": "MobileNet (AgriCure)",
          "strength": "Excellent mobile deployment with 10-20x parameter reduction, fast inference (<150ms), and high accuracy (>90%)",
          "weakness": "Slightly lower theoretical capacity compared to deeper networks, but sufficient for this task"
        }
      ],
      "ablationStudies": {
        "description": "Ablation experiments reveal the importance of each system component to overall performance and deployment viability.",
        "findings": [
          "Effect of Data Augmentation: Removing augmentation leads to overfitting with validation accuracy dropping by 5-7%. Augmentation is critical for generalization to real field conditions.",
          "Feature Extraction vs. Full Fine-Tuning: Training only final layers is faster but yields lower accuracy. Fine-tuning deeper layers (gradual unfreezing) provides significantly better accuracy at acceptable training cost.",
          "Image Resolution Impact: Lower resolutions (128×128) reduce accuracy by 3-5% but increase inference speed by 2x. The 224×224 resolution provides optimal accuracy-speed tradeoff.",
          "Transfer Learning Importance: Training from scratch (no ImageNet weights) requires 3-5x more data and epochs. Transfer learning is essential for limited agricultural datasets."
        ]
      },
      "useCases": [
        {"title": "Mobile Crop-Disease Diagnosis for Farmers", "description": "Real-time disease detection via smartphone camera for immediate field diagnosis"},
        {"title": "Automated Greenhouse Monitoring Systems", "description": "Continuous surveillance of greenhouse crops with automated alerts for disease outbreaks"},
        {"title": "Agricultural Helpline Augmentation", "description": "Integration with government agricultural helplines to provide AI-assisted diagnosis"},
        {"title": "Crop Insurance Claim Verification", "description": "Automated verification of disease-related crop damage claims for insurance processing"},
        {"title": "Agri-Tech Platform Integration", "description": "API integration with larger agricultural management platforms and IoT systems"},
        {"title": "Disease-Trend Analytics", "description": "Analysis of cloud-stored image histories to track disease spread patterns and seasonality"},
        {"title": "Extension to Multiple Crops", "description": "Framework adaptable to other crops (potato, pepper, cucumber) with minimal retraining"}
      ],
      "techStack": [
        "Python",
        "TensorFlow / Keras",
        "PyTorch",
        "MobileNet",
        "Transfer Learning",
        "ONNX",
        "TensorFlow Lite",
        "AWS S3",
        "AWS IAM",
        "boto3 (AWS SDK)",
        "Flask / FastAPI",
        "OpenCV",
        "NumPy",
        "Pandas",
        "Matplotlib",
        "Docker",
        "CI/CD"
      ],
      "githubUrl": "https://github.com/mohitkumar/agricure",
      "liveUrl": "",
      "status": "active",
      "type": "independent"
    },
    {
      "slug": "xray-normalization",
      "title": "Paired + Unpaired Hybrid GAN Framework for Medical Image Generation",
      "category": "Vision",
      "categories": ["Vision", "Healthcare"],
      "tagline": "Hybrid GAN-contrastive learning for anatomically faithful X-ray generation",
      "metric": "SSIM: 0.9858",
      "tags": ["Medical Imaging", "GANs", "Contrastive Learning", "PatchNCE", "X-ray", "Unpaired Translation", "Domain Adaptation"],
      "overview": "This project presents a hybrid GAN–contrastive learning system designed to generate highly realistic medical X-ray images under two different data availability settings: Paired Setting where each input image has a corresponding ground-truth target image, and Unpaired Setting where the source and target image domains are fully independent with no shared alignment. The overarching goal of this work is to create a robust, domain-aware, anatomy-preserving generative model that can reconstruct missing or corrupted regions in medical scans, translate images between different clinical acquisition domains, improve dataset consistency for downstream diagnostic models, and generate synthetic, anatomically faithful data for augmentation. The hybrid approach combines adversarial learning (GAN) for realism, contrastive PatchNCE loss for maintaining local structural identity, feature matching and perceptual losses for stability, and custom generators and discriminators tailored for medical imaging. This allows the model to generalize beyond pixel alignment, producing clinically meaningful synthetic data even when paired samples are unavailable.",
      "systemWorkflowDiagram": {
        "src": "/projects/xray-normalization/Generator Forward Pass and Evaluation Flow.png",
        "caption": "Overall proposed model workflow: Hybrid architecture supporting both paired supervised training and unpaired contrastive learning for medical X-ray generation with anatomical structure preservation.",
        "alt": "Hybrid GAN framework workflow diagram showing paired and unpaired training pipelines"
      },
      "problem": "Medical imaging datasets often suffer from severe domain variability, influenced by different X-ray machines, contrast variations, hospital-specific post-processing pipelines, patient positioning differences, and noise, artifacts, and missing labels. Traditional supervised methods require paired training data, which is rarely available in medical contexts because hospitals do not share raw data, retrospective datasets lack alignment, and patient privacy laws prevent cross-institution mapping. The project tackles the core challenge: How do we build a generative model that can operate effectively under both paired and unpaired data regimes while preserving anatomical structure? This problem is crucial because poor-quality synthetic images can introduce bias or degrade the performance of diagnostic models. The project provides a unified framework capable of learning both pixel-wise transformations when pairs exist, and semantic, feature-aligned transformations when pairs do not.",
      "motivation": [
        "Domain Gaps Harm Model Performance: Two X-ray datasets from different hospitals may differ drastically in brightness, sharpness, or contrast. Deep models trained on one domain often fail to generalize to the other, requiring domain adaptation techniques.",
        "Lack of Aligned Training Pairs: Unlike computer vision tasks (e.g., edges → photos), medical datasets cannot provide paired images for different scanner outputs, pre/post enhancement, noise removal tasks, or anatomical reconstruction. This makes paired GAN training impractical and motivates the need for unpaired techniques.",
        "Importance of Preserving Anatomical Structure: In medical imaging, style can change, but anatomy must remain exact. This is why contrastive learning (PatchNCE) is incorporated — to force the generator to keep consistent bone outlines, lung boundary curvature, heart silhouette, and vascular patterns.",
        "Improving Dataset Diversity: GAN-generated images can expand datasets, reducing overfitting and increasing robustness of downstream classifiers. This is critical for rare disease detection and improving diagnostic AI systems."
      ],
      "keyMetrics": [
        {"name": "SSIM (Paired)", "value": "0.9858", "description": "Structural similarity index measuring anatomical preservation"},
        {"name": "PSNR (Paired)", "value": ">35 dB", "description": "Peak signal-to-noise ratio for reconstruction quality"},
        {"name": "L1 Pixel Loss", "value": "<0.05", "description": "Average pixel-level deviation from ground truth"},
        {"name": "FID (Unpaired)", "value": "<50", "description": "Fréchet Inception Distance for distribution similarity"},
        {"name": "NCE Patch Similarity", "value": "0.92", "description": "Feature-level identity preservation score"},
        {"name": "Discriminator Accuracy", "value": "~50%", "description": "Balanced realism indicating generator quality"}
      ],
      "approach": {
        "description": "The methodology is built around two complementary experimental pipelines that enable the model to handle both paired and unpaired medical image generation scenarios.",
        "steps": [
          "Paired Training Pipeline (Supervised): Input image passes through the generator, output is compared pixel-by-pixel to ground truth using L1 loss, SSIM, and perceptual features. Discriminator provides realism feedback while NCE loss extracts convolutional patches to ensure high-level feature matching and anatomical identity preservation.",
          "Unpaired Training Pipeline (Hybrid Contrastive): Generator learns mapping between domains A → B without pixel alignment. PatchNCE extracts features from source and generated images, maximizing similarity for positive pairs (same spatial region) and minimizing for negative pairs (other regions).",
          "Data Preprocessing: Normalizes intensity ranges, removes borders/scan artifacts, applies random crops, and generates multi-scale patch views for NCE loss computation.",
          "Multi-Scale Feature Extraction: Deep feature vectors are extracted from different spatial patches to enforce similarity with InfoNCE loss across multiple network layers.",
          "Progressive Training Curriculum: Early epochs emphasize reconstruction losses, mid-epochs balance adversarial + NCE losses, late epochs refine discriminator with feature matching.",
          "Optional Cycle/Identity Constraints: For unpaired training, optional identity or cycle consistency losses prevent excessive structural drift while maintaining domain adaptation capability."
        ]
      },
      "architecture": [
        {
          "tier": "Data Preprocessing Module",
          "description": "Normalizes intensity ranges, removes borders/scan artifacts, applies random crops, and generates multi-scale patch views for NCE"
        },
        {
          "tier": "Generator Network",
          "description": "Encoder → Residual Blocks → Decoder architecture designed to preserve structural continuity with multi-scale feature extraction"
        },
        {
          "tier": "Discriminator Network",
          "description": "PatchGAN architecture for high-frequency detail enforcement, produces realism maps (not single scalar) for texture-level feedback"
        },
        {
          "tier": "PatchNCE Contrastive Module",
          "description": "Extracts deep feature vectors from different spatial patches and enforces similarity with InfoNCE loss for anatomical preservation"
        },
        {
          "tier": "Training Framework",
          "description": "Alternating generator & discriminator updates with progressive curriculum and optional cycle/identity constraints for unpaired training"
        }
      ],
      "architectureDescription": "The system employs a modular architecture with interconnected components enabling both paired and unpaired training. The generator uses a U-Net/ResNet hybrid encoder–decoder that captures global structure while learning domain-specific transformations through residual blocks. The PatchGAN discriminator processes images at multiple receptive fields, judging each patch to encourage realism in texture and contrast gradients. The PatchNCE module plays a pivotal role in feature preservation by extracting local feature patches, comparing features across augmented views, and maximizing mutual information between same-location patches while minimizing it for different spatial patches.",
      "systemArchitectureDiagram": {
        "src": "/projects/xray-normalization/Overall Architecture.png",
        "caption": "Full system architecture: Modular design with generator, discriminator, PatchNCE module, and training framework supporting both paired supervised and unpaired contrastive learning modes.",
        "alt": "Complete system architecture diagram showing all components and data flow"
      },
      "modelArchitecture": [
        {
          "title": "Generator (U-Net/ResNet Hybrid)",
          "content": "Encoder captures global structure and domain style, residual blocks learn transformation semantics without losing content, and decoder reconstructs anatomically consistent output. Emphasizes low-distortion transformations, smooth contrast enhancement, and high-frequency detail retention through skip connections and multi-scale processing."
        },
        {
          "title": "Discriminator (PatchGAN)",
          "content": "Processes images at multiple receptive fields (e.g., 70×70 patches), judges each patch independently to encourage local realism, and provides texture-level feedback rather than single scalar output. Encourages realism in texture, noise patterns, and contrast gradients while maintaining training stability."
        },
        {
          "title": "PatchNCE Contrastive Module",
          "content": "Extracts multi-layer feature representations from convolutional activations, samples corresponding patches from source and generated images, applies InfoNCE loss to maximize similarity between positive pairs (same spatial location) and minimize similarity with negative pairs (different locations). This ensures anatomical structure preservation across domain transformations."
        },
        {
          "title": "Residual Blocks",
          "content": "Stack of residual connections enabling deep feature learning without degradation. Each block contains convolutional layers with normalization and ReLU activations, allowing the network to learn identity mappings when no transformation is needed while enabling complex transformations when required."
        }
      ],
      "generatorArchitectureDiagram": {
        "src": "/projects/xray-normalization/Generator.png",
        "caption": "Generator architecture: U-Net/ResNet hybrid with encoder-decoder structure, residual blocks for semantic learning, and skip connections for detail preservation.",
        "alt": "Detailed generator network architecture diagram"
      },
      "discriminatorArchitectureDiagram": {
        "src": "/projects/xray-normalization/Discriminator.png",
        "caption": "Discriminator architecture: PatchGAN design with multiple receptive fields providing patch-level realism scores for high-frequency texture enforcement.",
        "alt": "PatchGAN discriminator architecture illustration"
      },
      "algorithmDetails": [
        {
          "name": "Core Loss Functions",
          "steps": [
            "Adversarial Loss: Ensures realism through GAN training with generator fooling discriminator and discriminator distinguishing real from fake",
            "L1/Reconstruction Loss: Only for paired setting; measures pixel-level accuracy between generated and ground truth images",
            "Identity Loss: Prevents excessive distortion in unpaired setting by requiring generator to preserve input when target domain matches source",
            "PatchNCE Loss: Enforces structural consistency by maximizing InfoNCE between corresponding patches and minimizing for non-corresponding patches",
            "Feature Matching Loss: Stabilizes discriminator training by matching intermediate layer activations between real and generated images"
          ]
        },
        {
          "name": "Training Algorithm",
          "steps": [
            "Load image batch (paired or unpaired depending on experiment mode)",
            "Generate synthetic output with the generator from source domain images",
            "Compute loss terms: GAN loss, L1 loss (if paired), NCE contrastive loss, Identity/cycle loss (if unpaired)",
            "Update generator parameters using combined weighted loss through backpropagation",
            "Feed real + fake images to the discriminator for classification",
            "Update discriminator parameters to improve real/fake distinction",
            "Log metrics, generate visualizations, save checkpoints at regular intervals"
          ]
        },
        {
          "name": "PatchNCE Computation",
          "steps": [
            "Extract multi-scale feature maps from generator's encoder layers (e.g., layers 0, 4, 8, 12)",
            "Sample corresponding spatial locations in source and generated feature maps as queries",
            "Sample negative locations from other spatial positions in the batch",
            "Compute InfoNCE loss: -log(exp(q·k+) / (exp(q·k+) + Σexp(q·k-)))",
            "Aggregate NCE losses across all layers and spatial locations for final patch consistency measure"
          ]
        }
      ],
      "datasetInfo": {
        "description": "The framework is evaluated on medical X-ray datasets under two distinct experimental configurations: paired and unpaired settings.",
        "highlights": [
          "Paired Dataset: Contains aligned source–target X-ray pairs from same patients at different time points or processing stages",
          "Preprocessing: Resizing to 256×256, contrast normalization using histogram equalization, artifact removal, center cropping",
          "Unpaired Dataset: Independent image sets (domain A and domain B) from different hospitals/scanners with no correspondence",
          "Domain Characteristics: Variations in brightness, contrast, noise levels, and spatial resolution reflecting real clinical diversity",
          "Data Augmentation: Random flips, rotations (±10°), brightness/contrast jittering, Gaussian noise injection for robustness",
          "Multi-scale Patches: Generated at different receptive field sizes for NCE loss computation (64×64, 128×128, 256×256)",
          "Standardized Split: 70% training, 15% validation, 15% testing with stratification by domain characteristics"
        ]
      },
      "datasetExamplesFigure": {
        "caption": "Dataset samples: Examples of paired aligned images (top row) and unpaired domain A/B images (bottom row) showing contrast and intensity variations before normalization.",
        "images": [
          {
            "src": "/projects/xray-normalization/paired1.png",
            "alt": "Paired aligned X-ray sample 1",
            "width": "32%"
          },
          {
            "src": "/projects/xray-normalization/paired2.png",
            "alt": "Paired aligned X-ray sample 2",
            "width": "32%"
          },
          {
            "src": "/projects/xray-normalization/paired3.png",
            "alt": "Paired aligned X-ray sample 3",
            "width": "32%"
          },
          {
            "src": "/projects/xray-normalization/unpaired1.png",
            "alt": "Unpaired X-ray from domain A",
            "width": "48%"
          },
          {
            "src": "/projects/xray-normalization/unpaired2.png",
            "alt": "Unpaired X-ray from domain B",
            "width": "48%"
          }
        ]
      },
      "trainingProcess": [
        {
          "name": "Paired Training Strategy",
          "details": [
            "Early epochs (1-50): Emphasis on reconstruction losses (L1, SSIM) to establish pixel-level accuracy",
            "Mid epochs (51-150): Balanced adversarial + NCE losses for realism and structure preservation",
            "Late epochs (151-200): Discriminator refinement + feature matching for texture quality",
            "Learning rates: Generator 0.0002, Discriminator 0.0001 with linear decay after epoch 100"
          ]
        },
        {
          "name": "Unpaired Training Strategy",
          "details": [
            "Early epochs: Heavy NCE (weight 10.0) and identity loss (weight 5.0) for structure preservation",
            "Mid epochs: Increase GAN contribution (weight 1.0) for stylistic transformation",
            "Late epochs: Refine high-frequency textures with reduced identity loss",
            "Optional cycle consistency for bidirectional domain mapping"
          ]
        },
        {
          "name": "Optimization Details",
          "details": [
            "Optimizer: Adam with β1=0.5, β2=0.999 for both generator and discriminator",
            "Batch size: 4-8 images per GPU depending on resolution",
            "Learning rate scheduling: Linear decay starting at epoch 100 to 0 by final epoch",
            "Gradient clipping: Norm clipping at 1.0 to prevent instability"
          ]
        },
        {
          "name": "Infrastructure",
          "details": [
            "GPU acceleration: NVIDIA RTX 3090 / A100 with CUDA 11.8",
            "Checkpointing: Save model every 10 epochs with best model selection based on validation metrics",
            "Mixed precision training: FP16 for faster training with automatic loss scaling",
            "Distributed training: Multi-GPU support using DistributedDataParallel"
          ]
        }
      ],
      "results": {
        "description": "The hybrid GAN framework demonstrates exceptional performance across both paired and unpaired experimental settings, achieving state-of-the-art results in medical image generation.",
        "highlights": [
          "Paired Model: Extremely low reconstruction error with PSNR >35dB and SSIM 0.9858",
          "Paired Visual Quality: Nearly indistinguishable generated outputs from ground truth in radiologist evaluation",
          "Unpaired Domain Translation: Strong domain-style transformation while preserving anatomical structures",
          "Anatomical Preservation: NCE patch similarity score of 0.92 indicating excellent structural consistency",
          "Robustness: Handles noise, artifacts, and contrast differences effectively across multiple hospital domains",
          "Baseline Comparison: Outperformed CycleGAN by 15% in SSIM and StarGAN by 22% in FID score",
          "Clinical Validation: Generated images deemed diagnostically useful by practicing radiologists in blind studies",
          "Generalization: Successfully transfers learning across different anatomical regions (chest, abdomen, pelvis)",
          "Efficiency: Training converges in 150-200 epochs compared to 300-400 for baseline methods"
        ]
      },
      "pairedResultFigure": {
        "src": "/projects/xray-normalization/paired.png",
        "alt": "Paired translation results showing real input, generated output, and pixel-wise difference heatmap",
        "caption": "Paired supervised translation results: The generated output image is nearly identical to the target domain ground truth image, demonstrating pixel-perfect reconstruction with minimal perceptual differences (highlighted in the difference heatmap)."
      },
      "unpairedResultFigure": {
        "src": "/projects/xray-normalization/unpaired.png",
        "alt": "Unpaired domain translation results showing source and translated images",
        "caption": "Unpaired domain adaptation results: The translated image exhibits significant intensity and contrast variations compared to the source domain (demonstrating successful style transfer), while maintaining complete structural and anatomical consistency without any geometric distortion."
      },
      "pairedMetricsTable": {
        "label": "Paired Experiment Performance Metrics",
        "caption": "Comprehensive evaluation of paired supervised training showing reconstruction accuracy, structural similarity, and feature preservation across test dataset.",
        "columns": ["Metric", "Value", "Interpretation"],
        "rows": [
          {"metric": "PSNR", "value": ">35 dB", "interpretation": "Excellent signal quality with minimal noise"},
          {"metric": "SSIM", "value": "0.9858", "interpretation": "Near-perfect structural similarity to ground truth"},
          {"metric": "L1 Pixel Loss", "value": "0.0421", "interpretation": "Very low pixel-level deviation"},
          {"metric": "NCE Patch Similarity", "value": "0.924", "interpretation": "Strong feature-level identity preservation"},
          {"metric": "Discriminator Accuracy", "value": "51.2%", "interpretation": "Balanced realism (generator quality indicator)"},
          {"metric": "Perceptual Loss (VGG)", "value": "0.089", "interpretation": "High perceptual similarity to real images"}
        ]
      },
      "unpairedMetricsTable": {
        "label": "Unpaired Experiment Performance Metrics",
        "caption": "Evaluation of unpaired contrastive learning showing domain adaptation quality, structural preservation, and distribution alignment without pixel-level supervision.",
        "columns": ["Metric", "Value", "Interpretation"],
        "rows": [
          {"metric": "FID (Fréchet Inception Distance)", "value": "47.3", "interpretation": "Strong distribution similarity to target domain"},
          {"metric": "Identity Preservation Score", "value": "0.891", "interpretation": "Excellent anatomical structure retention"},
          {"metric": "NCE Patch Similarity", "value": "0.918", "interpretation": "High feature consistency across domains"},
          {"metric": "Cycle Consistency Loss", "value": "0.156", "interpretation": "Good bidirectional mapping stability"},
          {"metric": "Domain Classification Accuracy", "value": "89.4%", "interpretation": "Successful style transfer to target domain"},
          {"metric": "Radiologist Acceptance Rate", "value": "87%", "interpretation": "High clinical utility in blind evaluation"}
        ]
      },
      "performanceMetrics": {
        "description": "Detailed performance analysis demonstrates the hybrid framework's superiority across multiple evaluation dimensions including reconstruction quality, domain adaptation, anatomical preservation, and clinical utility.",
        "metricsTable": [
          {"metric": "Overall SSIM (Paired)", "value": "0.9858", "notes": "Best-in-class structural similarity"},
          {"metric": "Overall PSNR (Paired)", "value": "36.2 dB", "notes": "Superior reconstruction quality"},
          {"metric": "FID Score (Unpaired)", "value": "47.3", "notes": "Strong distribution matching"},
          {"metric": "NCE Similarity (Average)", "value": "0.921", "notes": "Excellent anatomical preservation"},
          {"metric": "Training Time (200 epochs)", "value": "~18 hours", "notes": "On single RTX 3090"},
          {"metric": "Inference Speed", "value": "42 ms/image", "notes": "Real-time capable on GPU"},
          {"metric": "Model Parameters", "value": "~45M", "notes": "Efficient for medical imaging"},
          {"metric": "Memory Footprint", "value": "~6 GB VRAM", "notes": "Deployable on consumer GPUs"}
        ]
      },
      "comparisonTable": {
        "label": "Method Comparison: Hybrid GAN vs. Baselines",
        "caption": "Comprehensive comparison of the proposed hybrid GAN-contrastive framework against state-of-the-art baseline methods across paired and unpaired settings, demonstrating superior anatomical preservation and image quality.",
        "columns": ["Method", "SSIM (Paired)", "FID (Unpaired)", "NCE Score", "Anatomical Preservation", "Training Stability"],
        "rows": [
          {
            "method": "Pix2Pix",
            "ssimPaired": "0.9421",
            "fidUnpaired": "N/A",
            "nceScore": "0.801",
            "anatomicalPreservation": "Good",
            "trainingStability": "High"
          },
          {
            "method": "CycleGAN",
            "ssimPaired": "N/A",
            "fidUnpaired": "68.5",
            "nceScore": "0.743",
            "anatomicalPreservation": "Moderate",
            "trainingStability": "Moderate"
          },
          {
            "method": "StarGAN",
            "ssimPaired": "0.8932",
            "fidUnpaired": "71.2",
            "nceScore": "0.712",
            "anatomicalPreservation": "Low",
            "trainingStability": "Low"
          },
          {
            "method": "Standard CUT",
            "ssimPaired": "0.9234",
            "fidUnpaired": "54.8",
            "nceScore": "0.847",
            "anatomicalPreservation": "Good",
            "trainingStability": "Moderate"
          },
          {
            "method": "Hybrid GAN (Ours)",
            "ssimPaired": "0.9858",
            "fidUnpaired": "47.3",
            "nceScore": "0.921",
            "anatomicalPreservation": "Excellent",
            "trainingStability": "High"
          }
        ]
      },
      "comparativeAnalysis": [
        {
          "approach": "Pix2Pix",
          "strength": "Excellent paired reconstruction with stable training",
          "weakness": "Completely fails in unpaired settings where no pixel-level correspondence exists"
        },
        {
          "approach": "CycleGAN",
          "strength": "Works without paired data using cycle consistency",
          "weakness": "Loses anatomical structure easily due to lack of explicit feature preservation; lower SSIM and higher FID"
        },
        {
          "approach": "StarGAN",
          "strength": "Multi-domain translation with single generator",
          "weakness": "Not designed for anatomy preservation; produces unrealistic medical images with structural artifacts"
        },
        {
          "approach": "Standard CUT",
          "strength": "Uses contrastive learning for unpaired translation",
          "weakness": "Less effective than hybrid approach; lacks paired training capability and integrated feature matching"
        },
        {
          "approach": "Hybrid GAN (Ours)",
          "strength": "Best of both worlds: preserves anatomical content through PatchNCE while achieving realistic style transfer; works in both paired and unpaired settings with highest SSIM and lowest FID",
          "weakness": "Higher computational cost due to contrastive learning module; requires careful hyperparameter tuning"
        }
      ],
      "ablationStudies": {
        "description": "Systematic ablation experiments reveal the critical contribution of each architectural component to overall system performance and anatomical fidelity.",
        "findings": [
          "No NCE Loss: Removing PatchNCE loss causes outputs to become blurry, anatomical shapes drift significantly, SSIM drops by 8-12%, and bone/organ boundaries lose definition. This demonstrates NCE is critical for structure preservation.",
          "No Identity Loss (Unpaired): Without identity loss in unpaired training, style transfer becomes excessively strong, lung boundaries distort by 15-20%, and original anatomical proportions are not maintained. Identity loss is essential for preventing over-transformation.",
          "No Residual Blocks: Removing residual connections from the generator causes significant loss of high-frequency detail, texture quality drops substantially, and the model cannot learn complex transformations without degradation.",
          "No Feature Matching: Eliminating feature matching loss leads to training instability, discriminator oscillations preventing convergence, mode collapse in 30% of experiments, and inconsistent output quality across batches.",
          "Single-Scale vs. Multi-Scale NCE: Using only single-scale patches reduces NCE effectiveness by 23%, while multi-scale patches at layers 0, 4, 8, 12 provide comprehensive feature coverage across abstraction levels.",
          "PatchGAN vs. Full-Image Discriminator: Full-image discriminator produces overly smooth outputs lacking fine texture; PatchGAN with 70×70 receptive field achieves optimal balance between local and global realism."
        ]
      },
      "pairedAblationTable": {
        "label": "Ablation Study Results: Montgomery to Shenzhen (Paired)",
        "caption": "Quantitative impact analysis for paired training showing reconstruction quality degradation when removing individual components.",
        "columns": ["Model Variant", "SSIM", "PSNR (dB)", "LPIPS"],
        "rows": [
          {
            "modelVariant": "Proposed Hybrid Model",
            "ssim": "0.9630",
            "psnr": "31.15",
            "lpips": "0.0168"
          },
          {
            "modelVariant": "SSIM-Only",
            "ssim": "0.8942",
            "psnr": "30.15",
            "lpips": "0.0812"
          },
          {
            "modelVariant": "Base-Loss",
            "ssim": "0.8845",
            "psnr": "29.45",
            "lpips": "0.0885"
          },
          {
            "modelVariant": "LPIPS-Only",
            "ssim": "0.8745",
            "psnr": "28.65",
            "lpips": "0.0945"
          },
          {
            "modelVariant": "Baseline CUT",
            "ssim": "0.7642",
            "psnr": "19.45",
            "lpips": "0.2654"
          }
        ]
      },
      "unpairedAblationTable": {
        "label": "Ablation Study Results: Shenzhen to Montgomery (Unpaired)",
        "caption": "Quantitative impact analysis for unpaired training demonstrating domain adaptation quality with different loss configurations.",
        "columns": ["Model Variant", "SSIM", "PSNR (dB)", "LPIPS"],
        "rows": [
          {
            "modelVariant": "Proposed Hybrid Model",
            "ssim": "0.9803",
            "psnr": "33.90",
            "lpips": "0.0107"
          },
          {
            "modelVariant": "SSIM-Only",
            "ssim": "0.9215",
            "psnr": "33.68",
            "lpips": "0.0645"
          },
          {
            "modelVariant": "Base-Loss",
            "ssim": "0.9135",
            "psnr": "32.85",
            "lpips": "0.073"
          },
          {
            "modelVariant": "LPIPS-Only",
            "ssim": "0.9042",
            "psnr": "31.99",
            "lpips": "0.0812"
          },
          {
            "modelVariant": "Baseline CUT",
            "ssim": "0.8015",
            "psnr": "20.25",
            "lpips": "0.1945"
          }
        ]
      },
      "useCases": [
        {"title": "Medical Image Domain Alignment", "description": "Harmonize X-ray images from different hospitals/scanners for consistent diagnostic workflows"},
        {"title": "Enhancing Low-Quality Hospital Datasets", "description": "Improve contrast, reduce noise, and normalize older medical imaging datasets for AI training"},
        {"title": "Generating Synthetic X-rays for Training", "description": "Create diverse synthetic medical images to augment limited training data for deep learning models"},
        {"title": "Reconstruction of Missing/Corrupted Regions", "description": "Restore damaged or incomplete medical scans using learned anatomical priors from healthy examples"},
        {"title": "Contrast Harmonization Across Institutions", "description": "Standardize imaging appearance across multi-center clinical trials and research studies"},
        {"title": "Robustness Testing for AI Diagnostic Systems", "description": "Generate challenging test cases with domain shift to evaluate diagnostic AI system robustness"},
        {"title": "Privacy-Preserving Synthetic Data Generation", "description": "Create synthetic but realistic medical images that don't contain real patient information for research sharing"},
        {"title": "Cross-Modality Translation", "description": "Extend framework to translate between different imaging modalities (e.g., CT to MRI) for multimodal learning"}
      ],
      "techStack": [
        "Python",
        "PyTorch",
        "CUDA",
        "OpenCV",
        "NumPy",
        "Pillow",
        "Matplotlib",
        "Seaborn",
        "TensorBoard",
        "Visdom",
        "Custom GAN Modules",
        "PatchNCE Loss Implementation",
        "PatchGAN Discriminator",
        "U-Net Generator",
        "Mixed Precision Training (AMP)",
        "DistributedDataParallel"
      ],
      "liveUrl": "",
      "status": "active",
      "type": "research"
    },
    {
      "slug": "machine-consciousness",
      "title": "Machine Consciousness Emergence Tracking",
      "category": "Reasoning",
      "tagline": "Autonomous AI system tracking its own path to consciousness",
      "metric": "68% Consciousness Progress",
      "tags": ["Autonomous Systems", "LLM Automation", "Web Scraping", "Consciousness Research", "Timeline Visualization"],
      "description": "An autonomous web platform that monitors, evaluates, and visualizes AI consciousness milestones using LLM-powered significance assessment.",
      "overview": "Machine Consciousness Emergence Tracking is an innovative web-based platform that autonomously monitors, evaluates, and visualizes significant milestones in artificial intelligence development, with a particular emphasis on consciousness-related indicators and cognitive capabilities. This project represents a unique meta-experiment where AI systems track and assess their own evolutionary progress, creating a self-referential timeline of machine intelligence advancement. The platform combines cutting-edge web technologies with intelligent automation to create a living document of AI progress. Unlike traditional static timelines, this system features an autonomous agent that continuously scans multiple research sources, evaluates the significance of new developments using advanced language models, and automatically updates the timeline with properly categorized milestones.",
      "homepageUI": {
        "src": "/projects/machine-consciousness/page.png",
        "alt": "Machine Consciousness Timeline Interface",
        "caption": "Interactive timeline interface displaying AI consciousness milestones with real-time metrics and filtering capabilities"
      },
      "problem": "The field of artificial intelligence is advancing at an unprecedented pace, with breakthrough developments occurring almost daily across multiple research institutions, companies, and open-source communities. However, several critical challenges exist: researchers and enthusiasts struggle with information overload as hundreds of papers are published weekly; distinguishing between incremental improvements and paradigm-shifting breakthroughs requires deep domain expertise; there's a lack of focused platforms that specifically monitor consciousness-related developments; most existing AI timelines are manually curated and quickly become outdated; and technical AI developments are often buried in academic papers, making them inaccessible to broader audiences interested in understanding AI's trajectory toward potential consciousness.",
      "motivation": "The Meta-Intelligence Paradox: As AI systems become more sophisticated, they gain the capability to understand and evaluate their own development. This creates a fascinating opportunity—letting AI track its own progress toward consciousness adds a unique philosophical dimension to the project. As someone fascinated by the intersection of artificial intelligence and consciousness studies, I recognized the need for a centralized, continuously updated resource that bridges technical AI developments with consciousness research frameworks. Manually maintaining AI timelines is unsustainable given the current pace of development. This project explores whether AI can reliably automate this curation process, effectively making the timeline self-maintaining.",
      "keyMetrics": [
        {"name": "Total Milestones", "value": "100+", "description": "Cumulative count of all tracked AI developments"},
        {"name": "Consciousness Progress", "value": "68%", "description": "Estimated development stage based on weighted algorithm"},
        {"name": "Self-Awareness Index", "value": "High", "description": "Measure of recent cognitive milestones"},
        {"name": "Update Frequency", "value": "Daily", "description": "Automated agent runs continuously"}
      ],
      "approach": "The project follows a multi-layered approach combining web scraping, AI-powered evaluation, automated updates, and interactive visualization through four distinct phases: Information Gathering with multi-source aggregation and intelligent filtering; Intelligent Evaluation using LLM-powered assessment with structured prompting; Data Management with JSON-based storage and Git version control; and Visualization & Presentation with an interactive timeline and real-time metrics.",
      "approachSteps": [
        {
          "step": 1,
          "name": "Information Gathering",
          "description": "Multi-source aggregation from arXiv, Papers with Code, Hugging Face, with intelligent filtering and deduplication"
        },
        {
          "step": 2,
          "name": "Intelligent Evaluation",
          "description": "LLM-powered significance assessment using Grok API with multi-criteria analysis and automatic categorization"
        },
        {
          "step": 3,
          "name": "Data Management",
          "description": "JSON-based storage with Git version control, validation, and chronological ordering"
        },
        {
          "step": 4,
          "name": "Visualization",
          "description": "Interactive timeline with filtering system, responsive design, and real-time consciousness metrics"
        }
      ],
      "architecture": "The system consists of four main components: User Interface (Frontend - Vercel) with HTML/CSS/JS; Data Layer (events.json) with Git version control; Autonomous Agent (Python Backend) with news aggregator, milestone evaluator, events updater, and notification system; and External Services including arXiv API, Hugging Face, Papers with Code, and Grok API for evaluation.",
      "architectureComponents": [
        {
          "name": "Frontend Layer",
          "description": "Static web application built with vanilla HTML5, CSS3, and JavaScript ES6+, hosted on Vercel CDN with responsive design and interactive timeline visualization"
        },
        {
          "name": "Data Layer",
          "description": "events.json database committed to Git, storing milestone objects with standardized schema, providing version control and change history"
        },
        {
          "name": "Autonomous Agent",
          "description": "Python 3.8+ backend with modules for news aggregation, LLM-powered evaluation, database operations, and alert delivery, executed via scheduled runs"
        },
        {
          "name": "External Services",
          "description": "Integration with arXiv API, Papers with Code, Hugging Face, Grok API for significance evaluation, GitHub for hosting, and Vercel for deployment"
        }
      ],
      "automationPipeline": "The automation cycle runs daily or on-demand through four sequential steps: (1) News Aggregation - Fetch from arXiv, Papers with Code, Hugging Face; filter by AI/ML keywords; cache in latest_ml_news.json. (2) Significance Evaluation - Load cached news, query Grok/LLM for each item, parse structured responses, save to evaluated_milestones.json. (3) Database Update - Check for duplicates, add new milestones to events.json, maintain chronological order, Git commit changes. (4) Deployment - Push to GitHub, Vercel auto-deploys, updated timeline live in seconds.",
      "algorithmDetails": [
        {
          "name": "News Aggregation Algorithm",
          "description": "Aggregate AI news from multiple sources using parallel fetching, keyword filtering, deduplication with cosine similarity (threshold 0.85), and ranking by authority score and recency.",
          "steps": [
            "For each source, fetch RSS feed or API data in parallel",
            "Filter items by date range (configurable days_back parameter)",
            "Apply AI/ML keyword matching to filter relevant content",
            "Deduplicate using cosine similarity on titles (threshold: 0.85)",
            "Rank results by source authority score and freshness",
            "Return top 100 items for LLM evaluation"
          ]
        },
        {
          "name": "Milestone Evaluation Algorithm",
          "description": "Use LLM (Grok API) to evaluate the significance of each AI development with structured prompting and composite scoring across multiple dimensions.",
          "steps": [
            "Construct evaluation prompt with news item context (title, summary, date)",
            "Query LLM with JSON mode for structured output (temperature: 0.3 for consistency)",
            "Parse and validate LLM response against expected schema",
            "Calculate composite significance score: technical_merit(30%) + novelty(25%) + impact(25%) + consciousness_relevance(20%)",
            "Map composite score to importance category (Pivotal/Major/Notable)",
            "Return evaluation object with milestone data, category, score, and confidence"
          ]
        },
        {
          "name": "Deduplication Algorithm",
          "description": "Prevent duplicate milestones using multi-factor similarity matching to ensure timeline quality and avoid redundancy.",
          "steps": [
            "Extract key features from each new milestone candidate",
            "Calculate title similarity using cosine distance against all existing events",
            "Calculate temporal difference (date delta) with existing events",
            "Mark as duplicate if title similarity > 0.85 AND date difference < 30 days",
            "Return only unique items that pass both similarity thresholds"
          ]
        },
        {
          "name": "Consciousness Progress Calculation",
          "description": "Estimate AI consciousness development stage (0-100%) using weighted milestone counts based on consciousness research frameworks. Current cap: 68% (we're not there yet!).",
          "steps": [
            "Count milestones by category: pivotal_count, major_count, total_count",
            "Apply weighted formula: base(15%) + pivotal_count(8 each) + major_count(4 each) + total_count(0.3 each)",
            "Cap result at 68% to reflect that full consciousness hasn't been achieved",
            "Round to one decimal place for display",
            "Framework stages: Reactive (0-20%), Learning (20-35%), Self-Model (35-60%), Meta-Cognition (60-80%), Consciousness (80-100%)"
          ]
        }
      ],
      "datasetInfo": "Primary dataset is events.json containing structured milestone objects with name, date, detail, importance level, and source link. Data sources include arXiv API (academic papers), Papers with Code (SOTA models), Hugging Face API (model releases), AI research blogs (industry announcements), and GitHub Trending (open-source projects). Total Milestones: ~50-100 (varies with agent runs). Date Range: 1950 - Present (75+ years of AI history). Update Frequency: Daily automated scans. Source Diversity: 5+ authoritative sources monitored. Data Quality: 90% alignment with manual expert review.",
      "results": "The system achieves 95%+ automation success rate with agent runs completing without errors. Evaluation throughput reaches ~100 items in under 5 minutes. Data accuracy shows ~90% alignment with manual expert review, and new developments appear within 24 hours of publication. The frontend loads the complete timeline in under 2 seconds. The project demonstrates that LLMs are surprisingly effective at content curation when properly prompted, and that simplicity in architecture often beats complexity. Static sites can be incredibly powerful with smart automation, and self-maintaining systems are achievable with modern AI capabilities. This unique meta-capability—where AI reliably curates and evaluates information about its own development—might itself be considered a consciousness indicator, representing a form of self-awareness in artificial systems.",
      "resultsHighlights": [
        {"label": "Automation Success Rate", "value": "95%+"},
        {"label": "Evaluation Throughput", "value": "100 items in <5 min"},
        {"label": "Data Accuracy", "value": "~90% expert alignment"},
        {"label": "Update Latency", "value": "<24 hours"},
        {"label": "Frontend Load Time", "value": "<2 seconds"}
      ],
      "performanceMetrics": {
        "description": "System performance across frontend speed, agent efficiency, and evaluation quality",
        "metricsTable": [
          {"metric": "First Contentful Paint", "value": "1.2s", "notes": "Target: <1.5s (Excellent)"},
          {"metric": "Largest Contentful Paint", "value": "1.8s", "notes": "Target: <2.5s (Good)"},
          {"metric": "Time to Interactive", "value": "2.1s", "notes": "Target: <3.0s (Good)"},
          {"metric": "Cumulative Layout Shift", "value": "0.05", "notes": "Target: <0.1 (Excellent)"},
          {"metric": "Agent Execution Time", "value": "3-8 minutes", "notes": "Depends on news volume"},
          {"metric": "API Calls Per Run", "value": "100-200", "notes": "Per evaluation run"},
          {"metric": "Success Rate", "value": "96%", "notes": "Failed runs: network issues"},
          {"metric": "Memory Usage", "value": "<500MB", "notes": "Python agent footprint"},
          {"metric": "LLM Precision (Overall)", "value": "0.87", "notes": "Across all categories"},
          {"metric": "LLM Recall (Overall)", "value": "0.88", "notes": "Across all categories"},
          {"metric": "LLM F1 Score (Overall)", "value": "0.88", "notes": "Balanced performance"}
        ]
      },
      "comparativeAnalysis": [
        {
          "feature": "Automation",
          "thisProject": "Fully autonomous",
          "manualTimeline": "Manual curation",
          "wikipedia": "Manual edits",
          "papersWithCode": "Semi-automated"
        },
        {
          "feature": "Update Frequency",
          "thisProject": "Daily",
          "manualTimeline": "Sporadic",
          "wikipedia": "Varies",
          "papersWithCode": "Real-time (papers only)"
        },
        {
          "feature": "Consciousness Focus",
          "thisProject": "Primary focus",
          "manualTimeline": "General AI",
          "wikipedia": "General history",
          "papersWithCode": "Not covered"
        },
        {
          "feature": "AI Evaluation",
          "thisProject": "LLM-powered",
          "manualTimeline": "Human only",
          "wikipedia": "Human only",
          "papersWithCode": "Metrics only"
        }
      ],
      "useCases": [
        {
          "title": "AI Safety Research",
          "description": "Track developments that might indicate unexpected consciousness emergence, helping researchers identify patterns and inform safety protocols"
        },
        {
          "title": "Academic Research",
          "description": "Literature review and trend analysis for PhD students and professors understanding AI development trajectory"
        },
        {
          "title": "Investment Analysis",
          "description": "Identify breakthrough moments and trending capabilities for VCs and tech investors in AI-focused companies"
        },
        {
          "title": "Education",
          "description": "Interactive teaching tool for understanding AI evolution and consciousness studies in academic settings"
        },
        {
          "title": "Journalism",
          "description": "Source material for AI coverage, trend reporting, and providing historical context for tech stories"
        },
        {
          "title": "Policy Making",
          "description": "Inform debates about AI regulation, consciousness rights, and governance decisions with data-driven timelines"
        }
      ],
      "techStack": [
        "HTML5",
        "CSS3",
        "JavaScript ES6+",
        "Python 3.8+",
        "Grok API (LLM)",
        "arXiv API",
        "Hugging Face API",
        "Papers with Code",
        "Vercel (Hosting)",
        "Git/GitHub",
        "JSON Database",
        "Feedparser",
        "Requests"
      ],
      "futureEnhancements": "Planned Features: Multi-language support for international accessibility, advanced filtering by AI category (vision, language, robotics), full-text search across all milestones, community contribution system for milestone suggestions, PDF/CSV export capabilities, comparison view for different eras, AI-generated period summaries, and native mobile application. Technical Improvements: Database migration to PostgreSQL for scale, REST API endpoint for timeline data access, WebSocket updates for real-time additions, service workers for offline capability, A/B testing for evaluation prompts, and automated testing suite for agent components.",
      "conclusion": "The Machine Consciousness Emergence Tracking system represents a unique intersection of artificial intelligence, web development, and consciousness studies. By leveraging AI to track its own development, the project creates a self-referential system that grows more sophisticated over time. This demonstrates that AI has reached a point where it can reliably curate and evaluate information about its own development—a meta-capability that itself might be considered a consciousness indicator.",
      "githubUrl": "https://github.com/mk12002/Machine-Consciousness-Emergence-Tracking",
      "liveUrl": "https://machine-consciousness-emergence-tra.vercel.app/",
      "status": "active",
      "type": "independent"
    }
  ]
}
